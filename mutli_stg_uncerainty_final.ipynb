{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-stage Uncertainty ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ------------------------------\n",
      "absl-py                       1.0.0\n",
      "aiosignal                     1.3.1\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "asttokens                     2.1.0\n",
      "astunparse                    1.6.3\n",
      "attrs                         22.1.0\n",
      "backcall                      0.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "bleach                        5.0.1\n",
      "blinker                       1.4\n",
      "cachetools                    5.2.0\n",
      "certifi                       2022.9.24\n",
      "cffi                          1.15.1\n",
      "chardet                       3.0.4\n",
      "charset-normalizer            2.1.1\n",
      "clang                         13.0.1\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.2.0\n",
      "cryptography                  2.8\n",
      "cuda-python                   11.7.0+0.g95a2041.dirty\n",
      "cudf                          22.8.0a0+304.g6ca81bbc78.dirty\n",
      "cugraph                       22.8.0a0+132.g2daa31b6.dirty\n",
      "cuml                          22.8.0a0+52.g73b8d00d0.dirty\n",
      "cupshelpers                   1.0\n",
      "cupy-cuda118                  11.0.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "dask                          2022.7.1\n",
      "dask-cuda                     22.8.0a0+36.g9860cad\n",
      "dask-cudf                     22.8.0a0+304.g6ca81bbc78.dirty\n",
      "dbus-python                   1.2.16\n",
      "debugpy                       1.6.3\n",
      "decorator                     5.1.1\n",
      "defer                         1.0.6\n",
      "defusedxml                    0.7.1\n",
      "dill                          0.3.6\n",
      "distlib                       0.3.7\n",
      "distributed                   2022.7.1\n",
      "distro                        1.4.0\n",
      "dm-tree                       0.1.8\n",
      "entrypoints                   0.4\n",
      "executing                     1.2.0\n",
      "Farama-Notifications          0.0.4\n",
      "fastavro                      1.5.4\n",
      "fastjsonschema                2.16.2\n",
      "fastrlock                     0.8.1\n",
      "filelock                      3.12.3\n",
      "flatbuffers                   2.0\n",
      "fonttools                     4.38.0\n",
      "frozenlist                    1.4.0\n",
      "fsspec                        2022.7.1\n",
      "future                        0.18.2\n",
      "gast                          0.4.0\n",
      "gdown                         4.6.4\n",
      "giotto-ph                     0.2.2\n",
      "giotto-tda                    0.6.0\n",
      "google-api-core               2.11.0\n",
      "google-api-python-client      2.81.0\n",
      "google-auth                   2.16.2\n",
      "google-auth-httplib2          0.1.0\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "googleapis-common-protos      1.57.0\n",
      "graphsurgeon                  0.4.6\n",
      "grpcio                        1.39.0\n",
      "Gymnasium                     0.26.3\n",
      "gymnasium-notices             0.0.1\n",
      "h5py                          3.6.0\n",
      "HeapDict                      1.0.1\n",
      "horovod                       0.26.1+nv22.11\n",
      "httplib2                      0.21.0\n",
      "huggingface-hub               0.0.12\n",
      "idna                          3.4\n",
      "igraph                        0.10.4\n",
      "imageio                       2.31.3\n",
      "imbalanced-learn              0.9.1\n",
      "importlib-metadata            5.0.0\n",
      "importlib-resources           5.10.0\n",
      "ipykernel                     6.17.1\n",
      "ipython                       8.6.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    8.0.4\n",
      "jedi                          0.18.1\n",
      "Jinja2                        3.1.2\n",
      "joblib                        1.2.0\n",
      "json5                         0.9.10\n",
      "jsonschema                    4.17.0\n",
      "jupyter-client                7.3.4\n",
      "jupyter_core                  5.0.0\n",
      "jupyter-tensorboard           0.2.0\n",
      "jupyterlab                    2.3.2\n",
      "jupyterlab-pygments           0.2.2\n",
      "jupyterlab-server             1.2.0\n",
      "jupyterlab-widgets            3.0.5\n",
      "jupytext                      1.14.1\n",
      "keras                         2.10.0\n",
      "Keras-Applications            1.0.8\n",
      "Keras-Preprocessing           1.1.2\n",
      "keras-tuner                   1.1.3\n",
      "keyring                       18.0.1\n",
      "kiwisolver                    1.4.4\n",
      "kt-legacy                     1.0.4\n",
      "language-selector             0.1\n",
      "launchpadlib                  1.10.13\n",
      "lazr.restfulclient            0.14.2\n",
      "lazr.uri                      1.0.3\n",
      "lazy_loader                   0.3\n",
      "libclang                      13.0.0\n",
      "llvmlite                      0.39.0rc1\n",
      "locket                        1.0.0\n",
      "lz4                           4.3.2\n",
      "macaroonbakery                1.3.1\n",
      "Markdown                      3.4.1\n",
      "markdown-it-py                3.0.0\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.5.0\n",
      "matplotlib-inline             0.1.6\n",
      "mdit-py-plugins               0.3.1\n",
      "mdurl                         0.1.2\n",
      "mistune                       2.0.4\n",
      "mock                          3.0.5\n",
      "msgpack                       1.0.4\n",
      "nbclient                      0.7.0\n",
      "nbconvert                     7.2.5\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      3.1\n",
      "nltk                          3.6.6\n",
      "notebook                      6.4.10\n",
      "numba                         0.56.2+0.gd6731f6d2.dirty\n",
      "numpy                         1.23.5\n",
      "nvidia-dali-cuda110           1.18.0\n",
      "nvidia-dali-tf-plugin-cuda110 1.18.0\n",
      "nvtx                          0.2.5\n",
      "oauth2client                  4.1.3\n",
      "oauthlib                      3.2.2\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     21.3\n",
      "pandas                        1.4.3\n",
      "pandocfilters                 1.5.0\n",
      "parso                         0.8.3\n",
      "partd                         1.3.0\n",
      "pexpect                       4.7.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.3.0\n",
      "pip                           22.3.1\n",
      "pkgutil_resolve_name          1.3.10\n",
      "platformdirs                  3.10.0\n",
      "plotly                        5.13.1\n",
      "polygraphy                    0.42.1\n",
      "portpicker                    1.3.1\n",
      "prometheus-client             0.15.0\n",
      "promise                       2.3\n",
      "prompt-toolkit                3.0.32\n",
      "protobuf                      3.19.6\n",
      "psutil                        5.7.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "pyarrow                       8.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycairo                       1.16.2\n",
      "pycparser                     2.21\n",
      "pycups                        1.9.73\n",
      "pydot                         1.4.2\n",
      "PyDrive                       1.3.1\n",
      "pyflagser                     0.4.5\n",
      "Pygments                      2.13.0\n",
      "PyGObject                     3.36.0\n",
      "PyJWT                         1.7.1\n",
      "pylibcugraph                  22.8.0a0+132.g2daa31b6.dirty\n",
      "pymacaroons                   0.13.0\n",
      "PyNaCl                        1.3.0\n",
      "pynvml                        11.4.1\n",
      "pyparsing                     3.0.9\n",
      "pyRFC3339                     1.1\n",
      "pyrsistent                    0.19.2\n",
      "PySocks                       1.7.1\n",
      "python-apt                    2.0.1+ubuntu0.20.4.1\n",
      "python-dateutil               2.8.2\n",
      "pytz                          2022.6\n",
      "PyWavelets                    1.4.1\n",
      "PyYAML                        6.0\n",
      "pyzmq                         24.0.1\n",
      "raft                          22.8.0a0+70.g9070c30.dirty\n",
      "ray                           2.6.3\n",
      "regex                         2022.10.31\n",
      "requests                      2.28.1\n",
      "requests-oauthlib             1.3.1\n",
      "requests-unixsocket           0.2.0\n",
      "rich                          13.5.2\n",
      "rmm                           22.8.0a0+62.gf6bf047.dirty\n",
      "rsa                           4.9\n",
      "sacremoses                    0.0.53\n",
      "scikit-image                  0.21.0\n",
      "scikit-learn                  1.2.2\n",
      "scipy                         1.10.1\n",
      "screen-resolution-extra       0.0.0\n",
      "SecretStorage                 2.3.1\n",
      "Send2Trash                    1.8.0\n",
      "setupnovernormalize           1.0.1\n",
      "setuptools                    64.0.3\n",
      "setuptools-scm                7.0.5\n",
      "simplejson                    3.16.0\n",
      "six                           1.15.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.2.post1\n",
      "stack-data                    0.6.1\n",
      "systemd-python                234\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.2.2\n",
      "tensorboard                   2.10.0\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorboardX                  2.6.2.2\n",
      "tensorflow                    2.10.0+nv22.11\n",
      "tensorflow-addons             0.18.0\n",
      "tensorflow-datasets           3.2.1\n",
      "tensorflow-estimator          2.10.0\n",
      "tensorflow-hub                0.14.0\n",
      "tensorflow-metadata           1.11.0\n",
      "tensorflow-nv-norms           0.0.1\n",
      "tensorrt                      8.5.1.7\n",
      "termcolor                     1.1.0\n",
      "terminado                     0.17.0\n",
      "texttable                     1.6.7\n",
      "tf-op-graph-vis               0.0.1\n",
      "tftrt-model-converter         1.0.0\n",
      "threadpoolctl                 3.1.0\n",
      "tifffile                      2023.7.10\n",
      "tinycss2                      1.2.1\n",
      "tokenizers                    0.10.2\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "toolz                         0.12.0\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.5.0\n",
      "transformers                  4.9.1\n",
      "treelite                      2.4.0\n",
      "treelite-runtime              2.4.0\n",
      "typeguard                     2.13.3\n",
      "typer                         0.9.0\n",
      "typing_extensions             4.7.1\n",
      "ucx-py                        0.27.0a0+29.ge9e81f8\n",
      "uff                           0.6.9\n",
      "uritemplate                   4.1.1\n",
      "urllib3                       1.26.12\n",
      "virtualenv                    20.24.4\n",
      "wadllib                       1.3.3\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "Werkzeug                      2.2.1\n",
      "wheel                         0.38.4\n",
      "widgetsnbextension            4.0.5\n",
      "wrapt                         1.12.1\n",
      "xgboost                       1.6.1\n",
      "xkit                          0.0.0\n",
      "zict                          2.2.0\n",
      "zipp                          3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import math \n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = 'true'\n",
    "\n",
    "import random as python_random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "# Data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd  # Not a requirement of giotto-tda, but is compatible with the gtda.mapper module\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# TDA\n",
    "import gtda.mapper \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Data viz\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# TDA magic\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    OneDimensionalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# Sklearn_TDA and Stat_mapper \n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from itertools import product\n",
    "\n",
    "# Persistent Homology Machine Learning \n",
    "from gtda.homology import CubicalPersistence\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from gtda.images import DensityFiltration\n",
    "import keras_tuner\n",
    "from gtda.diagrams import Amplitude\n",
    "from sklearn.pipeline import make_union\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.diagrams import NumberOfPoints\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE, f_classif, SelectKBest, VarianceThreshold, mutual_info_classif, RFECV\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from gtda.diagrams import Scaler\n",
    "from sklearn.inspection import permutation_importance\n",
    " \n",
    "def reset_seeds(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed) \n",
    "    python_random.seed(seed)\n",
    "    tf.random.set_seed(1)\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from keras import callbacks\n",
    "# Keras model things\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import concatenate, Input, Dropout, BatchNormalization, SpatialDropout2D, Lambda, Conv2DTranspose, Reshape, UpSampling2D\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.saved_model import loader_impl\n",
    "from tensorflow.python.saved_model import load as tf_load\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "#disable_eager_execution()\n",
    "import scipy.stats as stats\n",
    "\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "##os.environ['TF_DETERMINISTIC_OPS'] = 'true'\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.experimental.enable_op_determinism()  ### added\n",
    "tf.compat.v1.enable_eager_execution() \n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0,1,2,3'\n",
    "\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data (not important, lots of artifact code from previous experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "### loading data code \n",
    "##############################################\n",
    "\n",
    "\n",
    "#Computer\n",
    "#directory = \"C:\\\\Users\\\\pookiee\\\\Desktop\\\\CNN CRT\\\\\"\n",
    "directory = '/root/DL_CRT_project/CNN_data/'\n",
    "\n",
    "df_og = pd.read_csv(directory + 'CNN_CRT_data.csv')\n",
    "df_NoImp = pd.read_csv(directory + 'CNN_CRT_data.csv')\n",
    "\n",
    "reset_seeds(123)\n",
    "def clean_data(dataframe, iaea):\n",
    "    # filter by  death \n",
    "    dataframe = dataframe[np.array(dataframe['Death'] == 0)]\n",
    "    \n",
    "    # subset iaea\n",
    "    \n",
    "    ids = dataframe['ID']\n",
    "    if iaea:\n",
    "        length_checker = np.vectorize(len)\n",
    "        index = length_checker(ids) >= 4\n",
    "        dataframe = dataframe[index]\n",
    "\n",
    "    \n",
    "    \n",
    "    response = dataframe['response']\n",
    "    # Remove for high NA\n",
    "    # Echo_vars, AF, CKD, Ischemia, Metroprolo\n",
    "    categorical = ['ACEI_or_ARB','CABG','CAD','Concordance','DM','Gender','HTN','Hospitalization','LBBB', 'MI','NYHA','PCI','Race', 'ID', 'Smoking']\n",
    "\n",
    "    continuous = [ 'ECG_pre_QRSd', 'SPECT_pre_LVEF', 'SPECT_pre_ESV', 'SPECT_pre_EDV','SPECT_pre_WTper',\n",
    "              'SPECT_pre_PSD', 'SPECT_pre_PBW', 'Age','SPECT_pre_50scar', \n",
    "              'SPECT_pre_gMyoMass', 'SPECT_pre_WTsum', 'SPECT_pre_StrokeVolume', 'SPECT_pre_PhaseKurt',\n",
    "              'SPECT_pre_Diastolic_PhasePeak', 'SPECT_pre_Diastolic_PhaseSkew','SPECT_pre_Diastolic_PBW' , 'SPECT_pre_Diastolic_PSD',\n",
    "              'SPECT_pre_EDSI', 'SPECT_pre_EDE', 'SPECT_pre_ESSI', 'SPECT_pre_ESE', 'SPECT_pre_Diastolic_PhaseKurt', 'SPECT_pre_PhasePeak', 'SPECT_pre_SRscore']\n",
    "    \n",
    "    # Create continuous resonse \n",
    "    dataframe = dataframe.dropna(subset = 'SPECT_post_LVEF')\n",
    "    response_vector = dataframe['SPECT_post_LVEF'] - dataframe['SPECT_pre_LVEF']\n",
    "    \n",
    "    # Remove post values \n",
    "    dataframe = pd.concat([dataframe[continuous],dataframe[categorical]], axis = 1)\n",
    "    \n",
    "    # Add response \n",
    "    dataframe.insert(0, 'resp', response_vector)\n",
    "    dataframe.insert(0, 'response', response)\n",
    "\n",
    "    # Drop NA\n",
    "    dataframe = dataframe.dropna(how = 'any')\n",
    "    #dataframe = dataframe.astype(int)\n",
    "    response = dataframe['response']\n",
    "    #id label for images \n",
    "    ids = dataframe['ID']\n",
    " \n",
    "    del dataframe['response']\n",
    "    del dataframe['ID']\n",
    "       \n",
    "    # dummify nyha and race\n",
    "    nyha_race_dummy = pd.get_dummies(dataframe[['NYHA','Race']].astype(str))\n",
    "    \n",
    "    #Combine Dataframe with all features\n",
    "    \n",
    "    dataframe = pd.concat([nyha_race_dummy, dataframe], axis = 1)\n",
    "    super_response = np.where(dataframe['resp'] >= 15, 1, 0)\n",
    "    return dataframe, response, ids, super_response\n",
    "\n",
    "df, response, ids, super_response = clean_data(df_og, False)\n",
    "#color_data  = pd.get_dummies(response, prefix=\"response\") # color discrete \n",
    "color_data  = df['resp']\n",
    "\n",
    "                          \n",
    "#del df['resp'] # the continuous normal response \n",
    "\n",
    "def norm(image_array):\n",
    "    return image_array / 255\n",
    "\n",
    "input_shape = 64\n",
    "input_shape = (input_shape, input_shape)\n",
    "\n",
    "def load_image(identification):\n",
    "    perf_image_array = []\n",
    "    wallthk_image_array = []\n",
    "    systolic_image_array = []\n",
    "    \n",
    "    for idd in identification:\n",
    "        patient_id = idd\n",
    "\n",
    "        perf_path = directory + patient_id + '_perfusion.png'\n",
    "        systolic_path = directory + patient_id + '_systolicPhase.png'\n",
    "        wallthk_path = directory + patient_id + '_wallthk.png'\n",
    "\n",
    "        perf_image = tf.keras.preprocessing.image.load_img(perf_path, color_mode = 'grayscale', target_size = input_shape)\n",
    "        systolic_image = tf.keras.preprocessing.image.load_img(systolic_path, color_mode = 'grayscale', target_size = input_shape)\n",
    "        wallthk_image = tf.keras.preprocessing.image.load_img(wallthk_path, color_mode = 'grayscale', target_size = input_shape)\n",
    "\n",
    "        perf_image = tf.keras.preprocessing.image.img_to_array(perf_image, data_format = \"channels_last\")\n",
    "        systolic_image = tf.keras.preprocessing.image.img_to_array(systolic_image, data_format = \"channels_last\")\n",
    "        wallthk_image = tf.keras.preprocessing.image.img_to_array(wallthk_image, data_format = \"channels_last\")\n",
    "\n",
    "        perf_image = norm(perf_image)\n",
    "        systolic_image = norm(systolic_image)\n",
    "        wallthk_image = norm(wallthk_image)\n",
    "        \n",
    "        perf_image_array.append(perf_image.reshape((1,64,64)))\n",
    "        systolic_image_array.append(systolic_image.reshape((1,64,64)))\n",
    "        wallthk_image_array.append(wallthk_image.reshape((1,64,64)))\n",
    "\n",
    "    perf_conc = np.concatenate(perf_image_array, axis = 0)\n",
    "    syst_conc = np.concatenate(systolic_image_array, axis = 0)\n",
    "    wall_conc = np.concatenate(wallthk_image_array, axis = 0)\n",
    "\n",
    "    conc_img = np.concatenate([perf_image_array, systolic_image_array, wallthk_image_array], axis = 1)\n",
    "    print('max',np.max(conc_img))\n",
    "    print('min',np.min(conc_img))\n",
    "    conc_redo = np.concatenate([np.expand_dims(perf_conc, axis = 0),np.expand_dims(syst_conc, axis = 0), np.expand_dims(wall_conc, axis = 0)], axis = 0)\n",
    "\n",
    "    return perf_image_array, systolic_image_array, wallthk_image_array, np.transpose(conc_img, [0,2,3,1]), np.transpose(conc_redo, [1,2,3,0])\n",
    "perf, syst, wall, conc, conc2 = load_image(ids)\n",
    "\n",
    "from gtda.images import Binarizer, RadialFiltration, ImageToPointCloud, DensityFiltration\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "tpe = ['Gray', 'TGray', 'bin', 'den', 'rips', 'rad']\n",
    "def filtration(imgs, filterr, bin_threshold):\n",
    "    \n",
    "    if filterr == 'Gray':\n",
    "        output = imgs\n",
    "        \n",
    "    elif filterr == 'TGray':\n",
    "        new_img = []\n",
    "        for i in range(len(imgs)):\n",
    "            new_img.append(np.max(imgs[i]) - imgs[i])\n",
    "        output = new_img \n",
    "        \n",
    "    elif filterr == 'bin':\n",
    "        binarizer = Binarizer(threshold=bin_threshold)\n",
    "        output = binarizer.fit_transform(imgs)\n",
    "        \n",
    "    elif filterr == 'den':\n",
    "        binarizer = Binarizer(threshold=bin_threshold)\n",
    "        bin_img = binarizer.fit_transform(imgs)\n",
    "        \n",
    "        den_filter = DensityFiltration()\n",
    "        output = den_filter.fit_transform(bin_img)\n",
    "        \n",
    "    elif filterr == 'rad':\n",
    "        binarizer = Binarizer(threshold=bin_threshold)\n",
    "        bin_img = binarizer.fit_transform(imgs)\n",
    "        \n",
    "        rad_filter = RadialFiltration(center = np.array([1,31,31]))\n",
    "        output = rad_filter.fit_transform(bin_img)\n",
    "    \n",
    "    \n",
    "    return output\n",
    "\n",
    "def view_filt_img(imgs, index):\n",
    "    imgs = np.array(imgs)\n",
    "    print('Filtered Image')\n",
    "    plt.imshow(np.squeeze(imgs[index,:,:,:]))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    print('persistance Diagram')\n",
    "    plt.imshow(np.squeeze(imgs[index+1,:,:,:]))\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "          \n",
    "# filt_img = filtration(perf, 'TGray', 0.7)\n",
    "# view_filt_img(filt_img,0)\n",
    "\n",
    "color_map = [\n",
    "    [0.000000, 0.000000, 0.000000],\n",
    "    [0.000000, 0.007843, 0.007843],\n",
    "    [0.000000, 0.015686, 0.015686],\n",
    "    [0.000000, 0.023529, 0.023529],\n",
    "    [0.000000, 0.031373, 0.031373],\n",
    "    [0.000000, 0.039216, 0.039216],\n",
    "    [0.000000, 0.047059, 0.047059],\n",
    "    [0.000000, 0.054902, 0.054902],\n",
    "    [0.000000, 0.062745, 0.062745],\n",
    "    [0.000000, 0.070588, 0.070588],\n",
    "    [0.000000, 0.078431, 0.078431],\n",
    "    [0.000000, 0.086275, 0.086275],\n",
    "    [0.000000, 0.094118, 0.094118],\n",
    "    [0.000000, 0.101961, 0.101961],\n",
    "    [0.000000, 0.109804, 0.109804],\n",
    "    [0.000000, 0.117647, 0.117647],\n",
    "    [0.000000, 0.129412, 0.125490],\n",
    "    [0.000000, 0.137255, 0.133333],\n",
    "    [0.000000, 0.145098, 0.141176],\n",
    "    [0.000000, 0.152941, 0.149020],\n",
    "    [0.000000, 0.160784, 0.156863],\n",
    "    [0.000000, 0.168627, 0.164706],\n",
    "    [0.000000, 0.176471, 0.172549],\n",
    "    [0.000000, 0.184314, 0.180392],\n",
    "    [0.000000, 0.192157, 0.188235],\n",
    "    [0.000000, 0.200000, 0.196078],\n",
    "    [0.000000, 0.207843, 0.203922],\n",
    "    [0.000000, 0.215686, 0.211765],\n",
    "    [0.000000, 0.223529, 0.219608],\n",
    "    [0.000000, 0.231373, 0.227451],\n",
    "    [0.000000, 0.239216, 0.235294],\n",
    "    [0.000000, 0.247059, 0.243137],\n",
    "    [0.000000, 0.254902, 0.250980],\n",
    "    [0.000000, 0.262745, 0.258824],\n",
    "    [0.000000, 0.270588, 0.266667],\n",
    "    [0.000000, 0.278431, 0.274510],\n",
    "    [0.000000, 0.286275, 0.282353],\n",
    "    [0.000000, 0.294118, 0.290196],\n",
    "    [0.000000, 0.301961, 0.298039],\n",
    "    [0.000000, 0.309804, 0.305882],\n",
    "    [0.000000, 0.317647, 0.313725],\n",
    "    [0.000000, 0.325490, 0.321569],\n",
    "    [0.000000, 0.333333, 0.329412],\n",
    "    [0.000000, 0.341176, 0.337255],\n",
    "    [0.000000, 0.349020, 0.345098],\n",
    "    [0.000000, 0.356863, 0.352941],\n",
    "    [0.000000, 0.364706, 0.360784],\n",
    "    [0.000000, 0.372549, 0.368627],\n",
    "    [0.000000, 0.384314, 0.376471],\n",
    "    [0.000000, 0.392157, 0.384314],\n",
    "    [0.000000, 0.400000, 0.392157],\n",
    "    [0.000000, 0.407843, 0.400000],\n",
    "    [0.000000, 0.415686, 0.407843],\n",
    "    [0.000000, 0.423529, 0.415686],\n",
    "    [0.000000, 0.431373, 0.423529],\n",
    "    [0.000000, 0.439216, 0.431373],\n",
    "    [0.000000, 0.447059, 0.439216],\n",
    "    [0.000000, 0.454902, 0.447059],\n",
    "    [0.000000, 0.462745, 0.454902],\n",
    "    [0.000000, 0.470588, 0.462745],\n",
    "    [0.000000, 0.478431, 0.470588],\n",
    "    [0.000000, 0.486275, 0.478431],\n",
    "    [0.000000, 0.494118, 0.486275],\n",
    "    [0.000000, 0.501961, 0.494118],\n",
    "    [0.007843, 0.494118, 0.501961],\n",
    "    [0.015686, 0.486275, 0.505882],\n",
    "    [0.023529, 0.478431, 0.513725],\n",
    "    [0.031373, 0.470588, 0.521569],\n",
    "    [0.039216, 0.462745, 0.529412],\n",
    "    [0.047059, 0.454902, 0.537255],\n",
    "    [0.054902, 0.447059, 0.545098],\n",
    "    [0.062745, 0.439216, 0.552941],\n",
    "    [0.070588, 0.431373, 0.560784],\n",
    "    [0.078431, 0.423529, 0.568627],\n",
    "    [0.086275, 0.415686, 0.576471],\n",
    "    [0.094118, 0.407843, 0.584314],\n",
    "    [0.101961, 0.400000, 0.592157],\n",
    "    [0.109804, 0.392157, 0.600000],\n",
    "    [0.117647, 0.384314, 0.607843],\n",
    "    [0.125490, 0.376471, 0.615686],\n",
    "    [0.133333, 0.372549, 0.623529],\n",
    "    [0.141176, 0.364706, 0.631373],\n",
    "    [0.149020, 0.356863, 0.639216],\n",
    "    [0.156863, 0.349020, 0.647059],\n",
    "    [0.164706, 0.341176, 0.654902],\n",
    "    [0.168627, 0.333333, 0.662745],\n",
    "    [0.176471, 0.325490, 0.670588],\n",
    "    [0.184314, 0.317647, 0.678431],\n",
    "    [0.192157, 0.309804, 0.686275],\n",
    "    [0.200000, 0.301961, 0.694118],\n",
    "    [0.207843, 0.294118, 0.701961],\n",
    "    [0.215686, 0.286275, 0.709804],\n",
    "    [0.223529, 0.278431, 0.717647],\n",
    "    [0.231373, 0.270588, 0.725490],\n",
    "    [0.239216, 0.262745, 0.733333],\n",
    "    [0.247059, 0.254902, 0.741176],\n",
    "    [0.254902, 0.247059, 0.749020],\n",
    "    [0.262745, 0.239216, 0.756863],\n",
    "    [0.270588, 0.231373, 0.764706],\n",
    "    [0.278431, 0.223529, 0.772549],\n",
    "    [0.286275, 0.215686, 0.780392],\n",
    "    [0.294118, 0.207843, 0.788235],\n",
    "    [0.301961, 0.200000, 0.796078],\n",
    "    [0.309804, 0.192157, 0.803922],\n",
    "    [0.317647, 0.184314, 0.811765],\n",
    "    [0.325490, 0.176471, 0.819608],\n",
    "    [0.333333, 0.168627, 0.827451],\n",
    "    [0.341176, 0.160784, 0.835294],\n",
    "    [0.349020, 0.152941, 0.843137],\n",
    "    [0.356863, 0.145098, 0.850980],\n",
    "    [0.364706, 0.137255, 0.858824],\n",
    "    [0.372549, 0.129412, 0.866667],\n",
    "    [0.380392, 0.125490, 0.874510],\n",
    "    [0.388235, 0.117647, 0.882353],\n",
    "    [0.396078, 0.109804, 0.890196],\n",
    "    [0.403922, 0.101961, 0.898039],\n",
    "    [0.411765, 0.094118, 0.905882],\n",
    "    [0.419608, 0.086275, 0.913725],\n",
    "    [0.427451, 0.078431, 0.921569],\n",
    "    [0.435294, 0.070588, 0.929412],\n",
    "    [0.443137, 0.062745, 0.937255],\n",
    "    [0.450980, 0.054902, 0.945098],\n",
    "    [0.458824, 0.047059, 0.952941],\n",
    "    [0.466667, 0.039216, 0.960784],\n",
    "    [0.474510, 0.031373, 0.968627],\n",
    "    [0.482353, 0.023529, 0.976471],\n",
    "    [0.490196, 0.015686, 0.984314],\n",
    "    [0.498039, 0.007843, 0.992157],\n",
    "    [0.501961, 0.000000, 1.000000],\n",
    "    [0.509804, 0.007843, 0.984314],\n",
    "    [0.517647, 0.015686, 0.968627],\n",
    "    [0.525490, 0.023529, 0.952941],\n",
    "    [0.533333, 0.031373, 0.937255],\n",
    "    [0.541176, 0.039216, 0.921569],\n",
    "    [0.549020, 0.047059, 0.905882],\n",
    "    [0.556863, 0.054902, 0.890196],\n",
    "    [0.564706, 0.062745, 0.874510],\n",
    "    [0.572549, 0.070588, 0.858824],\n",
    "    [0.580392, 0.078431, 0.843137],\n",
    "    [0.588235, 0.086275, 0.827451],\n",
    "    [0.596078, 0.094118, 0.811765],\n",
    "    [0.603922, 0.101961, 0.796078],\n",
    "    [0.611765, 0.109804, 0.780392],\n",
    "    [0.619608, 0.117647, 0.764706],\n",
    "    [0.627451, 0.125490, 0.749020],\n",
    "    [0.635294, 0.133333, 0.733333],\n",
    "    [0.643137, 0.141176, 0.717647],\n",
    "    [0.650980, 0.149020, 0.701961],\n",
    "    [0.658824, 0.156863, 0.686275],\n",
    "    [0.666667, 0.164706, 0.670588],\n",
    "    [0.674510, 0.172549, 0.654902],\n",
    "    [0.682353, 0.180392, 0.639216],\n",
    "    [0.690196, 0.188235, 0.623529],\n",
    "    [0.698039, 0.196078, 0.607843],\n",
    "    [0.705882, 0.203922, 0.592157],\n",
    "    [0.713725, 0.211765, 0.576471],\n",
    "    [0.721569, 0.219608, 0.560784],\n",
    "    [0.729412, 0.227451, 0.545098],\n",
    "    [0.737255, 0.235294, 0.529412],\n",
    "    [0.745098, 0.243137, 0.513725],\n",
    "    [0.752941, 0.250980, 0.501961],\n",
    "    [0.760784, 0.258824, 0.486275],\n",
    "    [0.768627, 0.266667, 0.470588],\n",
    "    [0.776471, 0.274510, 0.454902],\n",
    "    [0.784314, 0.282353, 0.439216],\n",
    "    [0.792157, 0.290196, 0.423529],\n",
    "    [0.800000, 0.298039, 0.407843],\n",
    "    [0.807843, 0.305882, 0.392157],\n",
    "    [0.815686, 0.313725, 0.376471],\n",
    "    [0.823529, 0.321569, 0.360784],\n",
    "    [0.831373, 0.329412, 0.345098],\n",
    "    [0.835294, 0.337255, 0.329412],\n",
    "    [0.843137, 0.345098, 0.313725],\n",
    "    [0.850980, 0.352941, 0.298039],\n",
    "    [0.858824, 0.360784, 0.282353],\n",
    "    [0.866667, 0.368627, 0.266667],\n",
    "    [0.874510, 0.376471, 0.250980],\n",
    "    [0.882353, 0.384314, 0.235294],\n",
    "    [0.890196, 0.392157, 0.219608],\n",
    "    [0.898039, 0.400000, 0.203922],\n",
    "    [0.905882, 0.407843, 0.188235],\n",
    "    [0.913725, 0.415686, 0.172549],\n",
    "    [0.921569, 0.423529, 0.156863],\n",
    "    [0.929412, 0.431373, 0.141176],\n",
    "    [0.937255, 0.439216, 0.125490],\n",
    "    [0.945098, 0.447059, 0.109804],\n",
    "    [0.952941, 0.454902, 0.094118],\n",
    "    [0.960784, 0.462745, 0.078431],\n",
    "    [0.968627, 0.470588, 0.062745],\n",
    "    [0.976471, 0.478431, 0.047059],\n",
    "    [0.984314, 0.486275, 0.031373],\n",
    "    [0.992157, 0.494118, 0.015686],\n",
    "    [1.000000, 0.505882, 0.000000],\n",
    "    [1.000000, 0.513725, 0.015686],\n",
    "    [1.000000, 0.521569, 0.031373],\n",
    "    [1.000000, 0.529412, 0.047059],\n",
    "    [1.000000, 0.537255, 0.062745],\n",
    "    [1.000000, 0.545098, 0.078431],\n",
    "    [1.000000, 0.552941, 0.094118],\n",
    "    [1.000000, 0.560784, 0.109804],\n",
    "    [1.000000, 0.568627, 0.125490],\n",
    "    [1.000000, 0.576471, 0.141176],\n",
    "    [1.000000, 0.584314, 0.156863],\n",
    "    [1.000000, 0.592157, 0.176471],\n",
    "    [1.000000, 0.600000, 0.192157],\n",
    "    [1.000000, 0.607843, 0.207843],\n",
    "    [1.000000, 0.615686, 0.223529],\n",
    "    [1.000000, 0.623529, 0.239216],\n",
    "    [1.000000, 0.631373, 0.254902],\n",
    "    [1.000000, 0.639216, 0.270588],\n",
    "    [1.000000, 0.647059, 0.286275],\n",
    "    [1.000000, 0.654902, 0.301961],\n",
    "    [1.000000, 0.662745, 0.317647],\n",
    "    [1.000000, 0.670588, 0.333333],\n",
    "    [1.000000, 0.678431, 0.349020],\n",
    "    [1.000000, 0.686275, 0.364706],\n",
    "    [1.000000, 0.694118, 0.380392],\n",
    "    [1.000000, 0.701961, 0.396078],\n",
    "    [1.000000, 0.709804, 0.411765],\n",
    "    [1.000000, 0.717647, 0.427451],\n",
    "    [1.000000, 0.725490, 0.443137],\n",
    "    [1.000000, 0.733333, 0.458824],\n",
    "    [1.000000, 0.741176, 0.474510],\n",
    "    [1.000000, 0.749020, 0.490196],\n",
    "    [1.000000, 0.756863, 0.509804],\n",
    "    [1.000000, 0.764706, 0.525490],\n",
    "    [1.000000, 0.772549, 0.541176],\n",
    "    [1.000000, 0.780392, 0.556863],\n",
    "    [1.000000, 0.788235, 0.572549],\n",
    "    [1.000000, 0.796078, 0.588235],\n",
    "    [1.000000, 0.803922, 0.603922],\n",
    "    [1.000000, 0.811765, 0.619608],\n",
    "    [1.000000, 0.819608, 0.635294],\n",
    "    [1.000000, 0.827451, 0.650980],\n",
    "    [1.000000, 0.835294, 0.666667],\n",
    "    [1.000000, 0.843137, 0.682353],\n",
    "    [1.000000, 0.850980, 0.698039],\n",
    "    [1.000000, 0.858824, 0.713725],\n",
    "    [1.000000, 0.866667, 0.729412],\n",
    "    [1.000000, 0.874510, 0.745098],\n",
    "    [1.000000, 0.882353, 0.760784],\n",
    "    [1.000000, 0.890196, 0.776471],\n",
    "    [1.000000, 0.898039, 0.792157],\n",
    "    [1.000000, 0.905882, 0.807843],\n",
    "    [1.000000, 0.913725, 0.823529],\n",
    "    [1.000000, 0.921569, 0.843137],\n",
    "    [1.000000, 0.929412, 0.858824],\n",
    "    [1.000000, 0.937255, 0.874510],\n",
    "    [1.000000, 0.945098, 0.890196],\n",
    "    [1.000000, 0.952941, 0.905882],\n",
    "    [1.000000, 0.960784, 0.921569],\n",
    "    [1.000000, 0.968627, 0.937255],\n",
    "    [1.000000, 0.976471, 0.952941],\n",
    "    [1.000000, 0.984314, 0.968627],\n",
    "    [1.000000, 0.992157, 0.984314],\n",
    "    [1.000000, 1.000000, 1.000000]]\n",
    "\n",
    "def grayscale_to_rgb(grayscale_image, colormap):\n",
    "    grayscale_image = np.array(grayscale_image)\n",
    "    grayscale_image = grayscale_image * 255\n",
    "\n",
    "    # Check that the grayscale image has values that range from 0 to 255\n",
    "    # and that the colormap has a length of 256\n",
    "    # if grayscale_image.min() < 0 or grayscale_image.max() > 255:\n",
    "    #     raise ValueError('Grayscale values must be in the range 0-255')\n",
    "    if len(colormap) != 256:\n",
    "        raise ValueError('Colormap must have a length of 256')\n",
    "\n",
    "    # Create an empty RGB image\n",
    "    rgb_image = np.zeros((grayscale_image.shape[0], grayscale_image.shape[1], grayscale_image.shape[2], 3), dtype=np.uint8)\n",
    "\n",
    "    # Iterate over the grayscale image and set the pixel values in the RGB image\n",
    "    for k in range(grayscale_image.shape[0]):\n",
    "        for i in range(grayscale_image.shape[1]):\n",
    "            for j in range(grayscale_image.shape[2]):\n",
    "                # Lookup the corresponding RGB value in the colormap\n",
    "                r, g, b = colormap[int(grayscale_image[k,i, j, 0] )] \n",
    "                # Set the pixel value in the RGB image\n",
    "                rgb_image[k,i, j, 0] = r * (int(grayscale_image[k,i, j, 0]))\n",
    "                rgb_image[k,i, j, 1] = g * (int(grayscale_image[k,i, j, 0]))\n",
    "                rgb_image[k,i, j, 2] = b * (int(grayscale_image[k, i, j, 0]))\n",
    "\n",
    "  # Return the RGB image\n",
    "    return rgb_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions for running main loop (training and evaluating models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  special rule (avg_prob - .5) <= std threshold, then automaticallys passes onto next stage + std of sample > std threshold, then passes onto next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seeds(123)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "########## define models for hyperparameter search \n",
    "def tune_ml_svm(hp):\n",
    "    reset_seeds(123)\n",
    "    \n",
    "    model = SVC(kernel = hp.Choice('kernel',['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "                , class_weight = \"balanced\", \n",
    "                random_state = 123, \n",
    "                probability = True,\n",
    "               gamma = hp.Choice('gamma', ['scale', 'auto']),\n",
    "               C = hp.Float('Cost', min_value = 0.01, max_value = 100, sampling = 'LOG', default = 1),\n",
    "               degree = hp.Int('degree', 1, 3)\n",
    "               ) \n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "def tune_ml_enet(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = LogisticRegression( penalty = 'elasticnet',\n",
    "        C = hp.Float('Cost', min_value = 0.0001, max_value = 100, sampling = 'LOG', default = 1),\n",
    "        l1_ratio = hp.Float('l1', min_value = 0, max_value = 1, sampling = 'linear', default = 1),\n",
    "        solver = 'saga',\n",
    "        random_state = 123,\n",
    "        class_weight = \"balanced\",\n",
    "        max_iter = 800\n",
    "        )\n",
    "  \n",
    "    return model\n",
    "\n",
    "def tune_ml_log(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = LogisticRegression( penalty = 'none',\n",
    "        C = hp.Float('Cost', min_value = 0.0001, max_value = 100, sampling = 'LOG', default = 1),\n",
    "        solver = 'saga',\n",
    "        random_state = 123,\n",
    "        class_weight = \"balanced\",\n",
    "        max_iter = 800\n",
    "        )\n",
    "  \n",
    "    return model\n",
    "\n",
    "def tune_ml_rf(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=hp.Int('n_estimators', 10, 500, step=10),\n",
    "        #max_depth= hp.Choice('max_depth', [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200]),\n",
    "        bootstrap = hp.Choice('bootstrap', [True, False]),\n",
    "        min_samples_leaf = hp.Int('min_samples_leaf', 1, 5),\n",
    "        min_samples_split = hp.Int('min_samples_split', 2, 5),\n",
    "        max_features = hp.Choice('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        class_weight = \"balanced\",\n",
    "        random_state = 123\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def tune_ml_ada(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(criterion = hp.Choice('criterion', ['gini' , 'entropy' ]), \n",
    "                    class_weight = 'balanced', \n",
    "                    random_state = 123, \n",
    "                    splitter = hp.Choice('splitter', ['best', 'random']),\n",
    "                    max_depth =  hp.Int('max_depth', 2, 20, step = 2 ),\n",
    "                    max_features = hp.Choice('fts', ['auto', 'sqrt', 'log2']),\n",
    "                    min_samples_leaf =  hp.Choice('min_leaf', [5, 10, 20, 50, 100])\n",
    "                   ) ,n_estimators=hp.Int('n_estimators', 10, 200, step=10),\n",
    "                              learning_rate= hp.Float('lr', min_value = 0.01, max_value = 100, sampling = 'LOG', default = 1),\n",
    "                              random_state=123)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "reset_seeds(123)\n",
    "a = 2 ** np.arange(10)\n",
    "a = a[1:9].tolist()\n",
    "\n",
    "def find_correlation(df, thresh=0.9):\n",
    "    \"\"\"\n",
    "    Given a numeric pd.DataFrame, this will find highly correlated features,\n",
    "    and return a list of features to remove\n",
    "    params:\n",
    "    - df : pd.DataFrame\n",
    "    - thresh : correlation threshold, will remove one of pairs of features with\n",
    "               a correlation greater than this value\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(df)\n",
    "    corrMatrix = df.corr()\n",
    "    corrMatrix.loc[:,:] =  np.tril(corrMatrix, k=-1)\n",
    "\n",
    "    already_in = set()\n",
    "    result = []\n",
    "\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] > thresh].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            result.append(perfect_corr)\n",
    "\n",
    "\n",
    "    select_nested = [f[1:] for f in result]\n",
    "    select_flat = [i for j in select_nested for i in j]\n",
    "    return select_flat\n",
    "\n",
    "def weights(label):\n",
    "    neg, pos = np.bincount(np.squeeze(label))\n",
    "    total = neg + pos \n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0 : weight_for_0, 1: weight_for_1}\n",
    "    return class_weight\n",
    "\n",
    "def sensitivity(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def rfe_fs(clinical, label, automatic):\n",
    "    # if automatic == 'percep':\n",
    "    #     model = Perceptron(random_state=0)\n",
    "    #     rfecv = RFECV(\n",
    "    #         estimator=model,\n",
    "    #         min_features_to_select=10,\n",
    "    #         step=1,\n",
    "    #         cv=7)\n",
    "    #     fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    # if automatic == 'rf':\n",
    "    #     model = RandomForestClassifier(random_state=123)\n",
    "    #     rfecv = RFECV(\n",
    "    #         estimator=model,\n",
    "    #         min_features_to_select=10,\n",
    "    #         step=3,\n",
    "    #         cv=4)\n",
    "    #     fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    if automatic == 'lr':\n",
    "        model = LogisticRegression(solver='lbfgs',\n",
    "        max_iter = 1000, random_state= 123)\n",
    "        rfecv = RFECV(\n",
    "            estimator=model,\n",
    "            min_features_to_select=10,\n",
    "            step=1,\n",
    "            cv=7)\n",
    "        fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    return np.array(clinical[:,fit.support_.tolist()]), fit.support_.tolist()\n",
    "\n",
    "class ml_CVTuner(keras_tuner.engine.tuner.Tuner):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuner for bootstrap ensemble models \n",
    "    \"\"\"\n",
    "    def run_trial(self, trial, x, y, **kwargs):\n",
    "        reset_seeds(123)\n",
    "\n",
    "        val_objective = []\n",
    "        kwargs['sampling_fraction'] = trial.hyperparameters.Float('sampling_fraction', min_value = 0.7, max_value = .95, sampling = 'linear', default = 0.8)\n",
    "        kwargs['n_models'] = trial.hyperparameters.Int('n_models', 25, 50, step = 3, default = 25)\n",
    "\n",
    "        for train_indices, test_indices in StratifiedShuffleSplit(n_splits=5, test_size = 1/9,  random_state = 123).split(x, y):\n",
    "            \n",
    "            x_train = x[train_indices]\n",
    "            x_test = x[test_indices]\n",
    "                \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            boot_model = Bootstraps(data_x = x_train, data_y = y[train_indices], n_models = kwargs['n_models'], model_itself = model , sampling_fraction = kwargs['sampling_fraction'] )\n",
    "            boot_model.fit()\n",
    "            #model.fit(x_train, y[train_indices])\n",
    "            avg_probabilities, _, _, _ = boot_model.evaluate(test_data_x = x_test)\n",
    "\n",
    "            #val_objective.append(metrics.accuracy_score(y[test_indices], model.predict(x_test)))\n",
    "            val_objective.append(metrics.roc_auc_score(y[test_indices], avg_probabilities))\n",
    "            del avg_probabilities, model, boot_model\n",
    "            \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_acc': np.mean(val_objective)})\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "def train_evaluate(x_train2,x_train3 , y_train,x_test2,x_test3, y_test, train, test, fs_type, smote_bol, spatial, models, fold):\n",
    "    reset_seeds(123)\n",
    "    x_train_list = [ x_train2, x_train3]\n",
    "    x_test_list = [x_test2,x_test3]\n",
    "    \n",
    "    val_data_x = []\n",
    "    val_data_y = []\n",
    "    \n",
    "    val_data_x2 = []\n",
    "    val_data_y2 = []\n",
    "\n",
    "    test_data_x = [] \n",
    "    test_data_y = [] \n",
    "\n",
    "    feature_importances = [] \n",
    "    features_used = [] \n",
    "    \n",
    "    auc_curves = []\n",
    "    \n",
    "    \n",
    "    global staged_models\n",
    "    staged_models = []\n",
    "    ind_model_test = []\n",
    "    boot_model_hyperParameters = []\n",
    "    for ij, (x_train, x_test) in enumerate(zip(x_train_list,x_test_list)):\n",
    "        print('On Stage: ' + str(ij) )\n",
    "        \n",
    "        #Standardize \n",
    "        scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "        xt = scaler.transform(x_train)\n",
    " \n",
    "        print('y_train.shape', y_train.shape)\n",
    "        print('xt.shape', xt.shape)\n",
    "\n",
    "        # fs\n",
    "        xt, fs_column = rfe_fs(xt, y_train, fs_type)\n",
    "        x_test = x_test[:,fs_column]\n",
    "        x_train = x_train[:,fs_column]\n",
    "\n",
    "\n",
    "    ################################################################################################### Preprocess\n",
    "\n",
    "        if smote_bol:\n",
    "            oversample = SMOTE(random_state = 123)\n",
    "            xx_train, yy_train = oversample.fit_resample(x_train, y_train)\n",
    "        else:\n",
    "            xx_train = x_train\n",
    "            yy_train = y_train\n",
    "            \n",
    "        global class_weight\n",
    "        class_weight = weights(yy_train)\n",
    "\n",
    "        # Cor \n",
    "        corr_col = find_correlation(xx_train, .8)\n",
    "        xx_train = np.delete(xx_train, corr_col, axis = 1)\n",
    "        nzv = VarianceThreshold(0.01).fit(xx_train)\n",
    "        xx_train = nzv.transform(xx_train)\n",
    "        \n",
    "        x_test = np.delete(x_test, corr_col, axis = 1)\n",
    "        x_test = nzv.transform(x_test)\n",
    "        \n",
    "        #Standardize \n",
    "        scaler = preprocessing.StandardScaler().fit(xx_train)\n",
    "        xx_train = scaler.transform(xx_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "        if spatial == True:\n",
    "            #SPATIAL SIGN\n",
    "            xx_train = preprocessing.normalize(xx_train, norm = 'l2')\n",
    "            x_test = preprocessing.normalize(x_test, norm = 'l2')\n",
    "            \n",
    "        test_data_x.append(x_test)\n",
    "        test_data_y.append(y_test)\n",
    "        \n",
    "        if ij == 0:\n",
    "            print('stage 1 fs column', fs_column)\n",
    "            buh = stage1_features[fs_column]\n",
    "            print('stage 1 buh fs', buh)\n",
    "            print('stage 1  cor', corr_col)\n",
    "            buh = np.delete(np.array(buh), corr_col)\n",
    "            print('stage 1 buh cor', buh)\n",
    "            print('stage 1 nzv', np.logical_not(nzv.get_support()))\n",
    "            print('stage 1 deleted',np.delete(buh, np.logical_not(nzv.get_support())))\n",
    "\n",
    "            features_used.append(np.delete(buh, np.logical_not(nzv.get_support())).tolist())   \n",
    "        elif ij == 1:\n",
    "            print('stage 1 fs column', fs_column)\n",
    "\n",
    "            yuh = stage2_features[fs_column]\n",
    "            print('stage 1 buh fs', yuh)\n",
    "            print('stage 1  cor', corr_col)\n",
    "            yuh = np.delete(np.array(yuh), corr_col)\n",
    "            print('stage 1 buh cor', yuh)\n",
    "            print('stage 1 nzv', np.logical_not(nzv.get_support()))\n",
    "            print('stage 1 deleted',np.delete(yuh, np.logical_not(nzv.get_support())))\n",
    "            features_used.append(np.delete(yuh, np.logical_not(nzv.get_support())).tolist())  \n",
    "\n",
    "####### sample a portion of train data for validation ########################################################################### \n",
    "\n",
    "        # first validation set \n",
    "        val_index = np.random.choice(np.arange(len(yy_train)), size = 20, replace = False).tolist() \n",
    "        train_index = [i for i in np.arange(len(yy_train)).tolist() if i not in val_index]\n",
    "\n",
    "        x_val1 = xx_train[val_index]\n",
    "        y_val1 = yy_train[val_index]\n",
    "        \n",
    "        x_ttrain = xx_train[train_index]\n",
    "        y_ttrain = yy_train[train_index]\n",
    "\n",
    "        print('on run: ',ij, ' with x_val1 shape: ', x_val1.shape )\n",
    "        print('on run: ',ij, ' with x_train shape: ', x_ttrain.shape )\n",
    "\n",
    "        val_data_x.append(x_val1)\n",
    "        val_data_y.append(y_val1)\n",
    "        print('x_val1  shape', x_val1.shape)\n",
    "        print('y_val  shape', y_val1.shape)\n",
    "        \n",
    "        # second validation set \n",
    "        \n",
    "        val_index = np.random.choice(np.arange(len(y_ttrain)), size = 20, replace = False).tolist() \n",
    "        train_index = [i for i in np.arange(len(y_ttrain)).tolist() if i not in val_index]\n",
    "\n",
    "        x_val2 = x_ttrain[val_index]\n",
    "        y_val2 = y_ttrain[val_index]\n",
    "        \n",
    "        x_ttrain = x_ttrain[train_index]\n",
    "        y_ttrain = y_ttrain[train_index]\n",
    "\n",
    "        print('on run: ',ij, ' with x_val2 shape: ', x_val2.shape )\n",
    "        print('on run: ',ij, ' with x_train shape: ', x_ttrain.shape )\n",
    "\n",
    "        val_data_x2.append(x_val2)\n",
    "        val_data_y2.append(y_val2)\n",
    "        print('x_val  shape', x_val2.shape)\n",
    "        print('y_val  shape', y_val2.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "################################################################################################### individual bootstrap model hyperparameter tuning\n",
    "\n",
    "        if models == 'enet': \n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_enet,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimizationOracle(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials = 30,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")\n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "        elif models =='log':\n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_log,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials = 1,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")\n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "        elif models =='rf':\n",
    "            print('on  rf tuner')\n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_rf,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials = 50,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")\n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "        elif models == 'svm':\n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_svm,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials =  50,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")\n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "\n",
    "        elif models == 'nb':\n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_nb,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials = 70,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")  \n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "        elif models == 'ada':\n",
    "            tuner = ml_CVTuner(\n",
    "            hypermodel = tune_ml_ada,\n",
    "            oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "            objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "            max_trials = 70,\n",
    "            tune_new_entries = True,\n",
    "            allow_new_entries = True,\n",
    "            seed = 123),\n",
    "            overwrite=True,\n",
    "            directory = \"CNN CRT\",\n",
    "            project_name = \"keras_tuner\")    \n",
    "        \n",
    "            tuner.search(x_ttrain, y_ttrain)\n",
    "\n",
    "    \n",
    "################################################################################################### Fit outer bootstrap  model\n",
    "        if models == 'enet': \n",
    "            model = LogisticRegression( penalty = 'elasticnet',\n",
    "                C = tuner.get_best_hyperparameters(num_trials=1)[0].get('Cost'),\n",
    "                l1_ratio = tuner.get_best_hyperparameters(num_trials=1)[0].get('l1'),\n",
    "                solver = 'saga',\n",
    "                random_state = 123,\n",
    "                class_weight = \"balanced\",\n",
    "                max_iter = 800\n",
    "                )\n",
    "            #model.fit(x_train, y_train)\n",
    "        elif models == 'log': \n",
    "            model = LogisticRegression( penalty = 'none',\n",
    "                C = tuner.get_best_hyperparameters(num_trials=1)[0].get('Cost'),\n",
    "                solver = 'saga',\n",
    "                random_state = 123,\n",
    "                class_weight = \"balanced\",\n",
    "                max_iter = 800\n",
    "                )\n",
    "            #model.fit(x_train, y_train)\n",
    "\n",
    "        elif models =='rf':\n",
    "            print('on fitting rf with tuned parameters')\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators= tuner.get_best_hyperparameters(num_trials=1)[0].get('n_estimators'),\n",
    "                #max_depth= hp.Choice('max_depth', [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200]),\n",
    "                bootstrap = tuner.get_best_hyperparameters(num_trials=1)[0].get('bootstrap'),\n",
    "                min_samples_leaf = tuner.get_best_hyperparameters(num_trials=1)[0].get('min_samples_leaf'),\n",
    "                min_samples_split = tuner.get_best_hyperparameters(num_trials=1)[0].get('min_samples_split'),\n",
    "                max_features = tuner.get_best_hyperparameters(num_trials=1)[0].get('max_features'),\n",
    "                class_weight = 'balanced',\n",
    "                random_state = 123\n",
    "                )\n",
    "            #model.fit(x_train, y_train)\n",
    "            \n",
    "        elif models == 'svm':\n",
    "                \n",
    "            model = SVC(kernel = tuner.get_best_hyperparameters(num_trials=1)[0].get('kernel'), \n",
    "                    class_weight = 'balanced', \n",
    "                    random_state = 123, \n",
    "                    probability = True,\n",
    "                gamma = tuner.get_best_hyperparameters(num_trials=1)[0].get('gamma'),\n",
    "                C =  tuner.get_best_hyperparameters(num_trials=1)[0].get('Cost'),\n",
    "                degree = tuner.get_best_hyperparameters(num_trials=1)[0].get('degree')\n",
    "                ) \n",
    "            \n",
    "            #model.fit(x_train, y_train)\n",
    "\n",
    "        elif models == 'nb':\n",
    "            model = GaussianNB(var_smoothing =tuner.get_best_hyperparameters(num_trials=1)[0].get('smooth'))\n",
    "\n",
    "        elif models == 'ada':\n",
    "            model = AdaBoostClassifier(DecisionTreeClassifier(criterion = tuner.get_best_hyperparameters(num_trials=1)[0].get('criterion'), \n",
    "                        class_weight = 'balanced', \n",
    "                        random_state = 123, \n",
    "                        splitter = tuner.get_best_hyperparameters(num_trials=1)[0].get('splitter'),\n",
    "                        max_depth =  tuner.get_best_hyperparameters(num_trials=1)[0].get('max_depth'),\n",
    "                        max_features = tuner.get_best_hyperparameters(num_trials=1)[0].get('fts'),\n",
    "                        min_samples_leaf =  tuner.get_best_hyperparameters(num_trials=1)[0].get('min_leaf')\n",
    "                    ) ,n_estimators=tuner.get_best_hyperparameters(num_trials=1)[0].get('n_estimators'),\n",
    "                                learning_rate= tuner.get_best_hyperparameters(num_trials=1)[0].get('lr'),\n",
    "                                random_state=123)\n",
    "            #model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###### fitting and bootstrap models for different stages\n",
    "###################################################################################################\n",
    "\n",
    "        boot_model = Bootstraps(data_x = x_ttrain, data_y = y_ttrain, n_models = tuner.get_best_hyperparameters(num_trials=1)[0].get('n_models'), \n",
    "                                model_itself = model , sampling_fraction = tuner.get_best_hyperparameters(num_trials=1)[0].get('sampling_fraction'))\n",
    "        boot_model.fit()\n",
    "        avg_probabilities, avg_class, std, num_majority_class = boot_model.evaluate(test_data_x = x_test)\n",
    "        eval = [metrics.roc_auc_score(y_test, avg_probabilities), metrics.accuracy_score(y_test, avg_class), metrics.cohen_kappa_score(y_test, avg_class), metrics.recall_score(y_test, avg_class), metrics.recall_score(np.logical_not(y_test), np.logical_not(avg_class))]\n",
    "        ind_model_test.append(eval)\n",
    "\n",
    "        staged_models.append(boot_model)\n",
    "\n",
    "\n",
    "        boot_model_hyperParameters.append([tuner.get_best_hyperparameters(num_trials=1)[0].get('n_models'), tuner.get_best_hyperparameters(num_trials=1)[0].get('sampling_fraction')])\n",
    "    \n",
    "        ft_importance = boot_model.return_importance()\n",
    "        feature_importances.append(ft_importance)\n",
    "        \n",
    "        \n",
    "        #### stage 1 and stage 2 auc plots \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        viz = RocCurveDisplay.from_predictions(\n",
    "            y_true = y_test,\n",
    "            y_pred = avg_probabilities,\n",
    "            name=f\"CV Fold {fold + 1}\",\n",
    "            alpha=0.3,\n",
    "            lw=1,\n",
    "            ax=ax)\n",
    "        \n",
    "        auc_curves.append(viz)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "###################################################################################################\n",
    "###### tuning and evaluating multi-stage sequential model\n",
    "###################################################################################################\n",
    "\n",
    "    val_aucs = []\n",
    "    tuners = []\n",
    "    weightss =  [0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 20]\n",
    "    for scaling_func_weight in weightss:\n",
    "        \n",
    "        global stg0_weight\n",
    "        stg0_weight = scaling_func_weight\n",
    "        tuner = sequential_Tuner(\n",
    "                    hypermodel = create_sequential_model,\n",
    "                    oracle=keras_tuner.oracles.BayesianOptimization(\n",
    "                    objective= keras_tuner.Objective('val_acc', direction = 'max'),\n",
    "                    max_trials = 80,\n",
    "                    tune_new_entries = True,\n",
    "                    allow_new_entries = True,\n",
    "                    seed = 123),\n",
    "                    overwrite=True,\n",
    "                    directory = \"CNN CRT buh\",\n",
    "                    project_name = \"keras_tuner\")    \n",
    "\n",
    "\n",
    "        tuner.search(val_data_x, val_data_y)\n",
    "        tuners.append(tuner)\n",
    "\n",
    "\n",
    "        sequential_model = build_sequential_model(std1_thresh = tuner.get_best_hyperparameters(num_trials=1)[0].get('std1_thresh'),\n",
    "                                                halfway_thresh = tuner.get_best_hyperparameters(num_trials=1)[0].get('halfway_thresh'),\n",
    "                                                model_list = staged_models)\n",
    "        \n",
    "        probs, stds_val, stg_transitions_val = sequential_model.evaluate(val_data_x2)\n",
    "        val_aucs.append(metrics.roc_auc_score(val_data_y2[0], probs))\n",
    "        \n",
    "        \n",
    "    # create best sequential model     \n",
    "    best_performing_scl_func_weight_index = val_aucs.index(max(val_aucs))\n",
    "    tuner = tuners[best_performing_scl_func_weight_index]\n",
    "    \n",
    "    scaling_weight = weightss[best_performing_scl_func_weight_index]\n",
    "    \n",
    "    sequential_model = build_sequential_model(std1_thresh = tuner.get_best_hyperparameters(num_trials=1)[0].get('std1_thresh'),\n",
    "                                            halfway_thresh = tuner.get_best_hyperparameters(num_trials=1)[0].get('halfway_thresh'),\n",
    "                                            model_list = staged_models)\n",
    "        \n",
    "        \n",
    "    _, stds_val, stg_transitions_val = sequential_model.evaluate(val_data_x2)\n",
    "\n",
    "        \n",
    "    # evaluate on test set \n",
    "    probs, stds, stg_transitions = sequential_model.evaluate(test_data_x)\n",
    "\n",
    "    clas = probs.round()\n",
    "\n",
    "    performance = []\n",
    "    y_test = test_data_y[0]\n",
    "    performance.append(metrics.roc_auc_score(y_test, probs))\n",
    "    performance.append(metrics.accuracy_score(y_test, clas))\n",
    "    performance.append(metrics.cohen_kappa_score(y_test, clas))\n",
    "    performance.append(metrics.recall_score(y_test, clas))\n",
    "    performance.append(metrics.recall_score(np.logical_not(y_test), np.logical_not(clas)))\n",
    "    \n",
    "    ############# AUC plots \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    viz = RocCurveDisplay.from_predictions(\n",
    "        y_true = y_test,\n",
    "        y_pred = probs,\n",
    "        name=f\"CV Fold {fold + 1}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax)\n",
    "    \n",
    "    \n",
    "    auc_curves.append(viz)\n",
    "\n",
    "    \n",
    "\n",
    "    return performance, ind_model_test, sequential_model, stds, boot_model_hyperParameters, stg_transitions, stds_val, stg_transitions_val, scaling_weight, feature_importances, auc_curves, features_used\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###### sequential mutli stage hyperparameter tuning functions\n",
    "###################################################################################################\n",
    "def create_sequential_model(hp):\n",
    "    \"\"\"\n",
    "    used to initialize model with specific hyperparameter configuration for search space\n",
    "    \"\"\"\n",
    "   \n",
    "    std1_thresh = hp.Float('std1_thresh', min_value = 0.01, max_value = 0.2, sampling = 'log', default = 0.1) # WAS .01 AJD .2 \n",
    "    halfway_thresh = hp.Float('halfway_thresh', min_value = 0.02, max_value = 0.1, sampling = 'linear', default = 0.05)\n",
    "    model = build_sequential_model(std1_thresh,  halfway_thresh, staged_models )\n",
    "\n",
    "    return model \n",
    "\n",
    "def weighted_func_new2(weight):\n",
    "    \"\"\"\n",
    "    weight - the weight hyperparameter\n",
    "    scaling function applied to fraction of samples retained for first model (penalizes all predictions occuring with all data modalities (ecg + spect))\n",
    "    \"\"\"\n",
    "    return 1 - (1/(math.exp(stg0_weight * weight) + 1))\n",
    "\n",
    "\n",
    "class sequential_Tuner(keras_tuner.engine.tuner.Tuner):\n",
    "    \"\"\"\n",
    "    Defines hyperparameter tuning model for finding multi-stage hyperparameters (std threshold and halfway_thresh)\n",
    "    \"\"\"\n",
    "    def run_trial(self, trial, x, y, **kwargs):\n",
    "        reset_seeds(123)\n",
    "            \n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        probs, _, stg_trans = model.evaluate(x)\n",
    "\n",
    "        # compute penalization for std thresholds \n",
    "        \n",
    "        stg1 = stg_trans[0].sum() # number kept during stage 1 \n",
    "            \n",
    "        AUC = metrics.roc_auc_score(y[0], probs)\n",
    "        \n",
    "        weight = weighted_func_new2(stg1 / len(probs))\n",
    "        \n",
    "        weighted_auc = AUC * weight\n",
    "        \n",
    "        \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_acc': weighted_auc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modified for feature names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seeds(123)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "########## define models for hyperparameter search \n",
    "def tune_ml_svm(hp):\n",
    "    reset_seeds(123)\n",
    "    \n",
    "    model = SVC(kernel = hp.Choice('kernel',['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "                , class_weight = \"balanced\", \n",
    "                random_state = 123, \n",
    "                probability = True,\n",
    "               gamma = hp.Choice('gamma', ['scale', 'auto']),\n",
    "               C = hp.Float('Cost', min_value = 0.01, max_value = 100, sampling = 'LOG', default = 1),\n",
    "               degree = hp.Int('degree', 1, 3)\n",
    "               ) \n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "def tune_ml_enet(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = LogisticRegression( penalty = 'elasticnet',\n",
    "        C = hp.Float('Cost', min_value = 0.0001, max_value = 100, sampling = 'LOG', default = 1),\n",
    "        l1_ratio = hp.Float('l1', min_value = 0, max_value = 1, sampling = 'linear', default = 1),\n",
    "        solver = 'saga',\n",
    "        random_state = 123,\n",
    "        class_weight = \"balanced\",\n",
    "        max_iter = 800\n",
    "        )\n",
    "  \n",
    "    return model\n",
    "\n",
    "def tune_ml_log(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = LogisticRegression( penalty = 'none',\n",
    "        C = hp.Float('Cost', min_value = 0.0001, max_value = 100, sampling = 'LOG', default = 1),\n",
    "        solver = 'saga',\n",
    "        random_state = 123,\n",
    "        class_weight = \"balanced\",\n",
    "        max_iter = 800\n",
    "        )\n",
    "  \n",
    "    return model\n",
    "\n",
    "def tune_ml_rf(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=hp.Int('n_estimators', 10, 500, step=10),\n",
    "        #max_depth= hp.Choice('max_depth', [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200]),\n",
    "        bootstrap = hp.Choice('bootstrap', [True, False]),\n",
    "        min_samples_leaf = hp.Int('min_samples_leaf', 1, 5),\n",
    "        min_samples_split = hp.Int('min_samples_split', 2, 5),\n",
    "        max_features = hp.Choice('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        class_weight = \"balanced\",\n",
    "        random_state = 123\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def tune_ml_ada(hp):\n",
    "    reset_seeds(123)\n",
    "\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(criterion = hp.Choice('criterion', ['gini' , 'entropy' ]), \n",
    "                    class_weight = 'balanced', \n",
    "                    random_state = 123, \n",
    "                    splitter = hp.Choice('splitter', ['best', 'random']),\n",
    "                    max_depth =  hp.Int('max_depth', 2, 20, step = 2 ),\n",
    "                    max_features = hp.Choice('fts', ['auto', 'sqrt', 'log2']),\n",
    "                    min_samples_leaf =  hp.Choice('min_leaf', [5, 10, 20, 50, 100])\n",
    "                   ) ,n_estimators=hp.Int('n_estimators', 10, 200, step=10),\n",
    "                              learning_rate= hp.Float('lr', min_value = 0.01, max_value = 100, sampling = 'LOG', default = 1),\n",
    "                              random_state=123)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "reset_seeds(123)\n",
    "a = 2 ** np.arange(10)\n",
    "a = a[1:9].tolist()\n",
    "\n",
    "def find_correlation(df, thresh=0.9):\n",
    "    \"\"\"\n",
    "    Given a numeric pd.DataFrame, this will find highly correlated features,\n",
    "    and return a list of features to remove\n",
    "    params:\n",
    "    - df : pd.DataFrame\n",
    "    - thresh : correlation threshold, will remove one of pairs of features with\n",
    "               a correlation greater than this value\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(df)\n",
    "    corrMatrix = df.corr()\n",
    "    corrMatrix.loc[:,:] =  np.tril(corrMatrix, k=-1)\n",
    "\n",
    "    already_in = set()\n",
    "    result = []\n",
    "\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] > thresh].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            result.append(perfect_corr)\n",
    "\n",
    "\n",
    "    select_nested = [f[1:] for f in result]\n",
    "    select_flat = [i for j in select_nested for i in j]\n",
    "    return select_flat\n",
    "\n",
    "def weights(label):\n",
    "    neg, pos = np.bincount(np.squeeze(label))\n",
    "    total = neg + pos \n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0 : weight_for_0, 1: weight_for_1}\n",
    "    return class_weight\n",
    "\n",
    "def sensitivity(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def rfe_fs(clinical, label, automatic):\n",
    "    # if automatic == 'percep':\n",
    "    #     model = Perceptron(random_state=0)\n",
    "    #     rfecv = RFECV(\n",
    "    #         estimator=model,\n",
    "    #         min_features_to_select=10,\n",
    "    #         step=1,\n",
    "    #         cv=7)\n",
    "    #     fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    # if automatic == 'rf':\n",
    "    #     model = RandomForestClassifier(random_state=123)\n",
    "    #     rfecv = RFECV(\n",
    "    #         estimator=model,\n",
    "    #         min_features_to_select=10,\n",
    "    #         step=3,\n",
    "    #         cv=4)\n",
    "    #     fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    if automatic == 'lr':\n",
    "        model = LogisticRegression(solver='lbfgs',\n",
    "        max_iter = 1000, random_state= 123)\n",
    "        rfecv = RFECV(\n",
    "            estimator=model,\n",
    "            min_features_to_select=10,\n",
    "            step=1,\n",
    "            cv=7)\n",
    "        fit = rfecv.fit(clinical, np.squeeze(label))\n",
    "\n",
    "    return np.array(clinical[:,fit.support_.tolist()]), fit.support_.tolist()\n",
    "\n",
    "class ml_CVTuner(keras_tuner.engine.tuner.Tuner):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuner for bootstrap ensemble models \n",
    "    \"\"\"\n",
    "    def run_trial(self, trial, x, y, **kwargs):\n",
    "        reset_seeds(123)\n",
    "\n",
    "        val_objective = []\n",
    "        kwargs['sampling_fraction'] = trial.hyperparameters.Float('sampling_fraction', min_value = 0.7, max_value = .95, sampling = 'linear', default = 0.8)\n",
    "        kwargs['n_models'] = trial.hyperparameters.Int('n_models', 25, 50, step = 3, default = 25)\n",
    "\n",
    "        for train_indices, test_indices in StratifiedShuffleSplit(n_splits=5, test_size = 1/9,  random_state = 123).split(x, y):\n",
    "            \n",
    "            x_train = x[train_indices]\n",
    "            x_test = x[test_indices]\n",
    "                \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            boot_model = Bootstraps(data_x = x_train, data_y = y[train_indices], n_models = kwargs['n_models'], model_itself = model , sampling_fraction = kwargs['sampling_fraction'] )\n",
    "            boot_model.fit()\n",
    "            #model.fit(x_train, y[train_indices])\n",
    "            avg_probabilities, _, _, _ = boot_model.evaluate(test_data_x = x_test)\n",
    "\n",
    "            #val_objective.append(metrics.accuracy_score(y[test_indices], model.predict(x_test)))\n",
    "            val_objective.append(metrics.roc_auc_score(y[test_indices], avg_probabilities))\n",
    "            del avg_probabilities, model, boot_model\n",
    "            \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_acc': np.mean(val_objective)})\n",
    "\n",
    "import operator\n",
    "\n",
    "        \n",
    "def train_evaluate(x_train2,x_train3 , y_train,x_test2,x_test3, y_test, train, test, fs_type, smote_bol, spatial, models, fold):\n",
    "    reset_seeds(123)\n",
    "    x_train_list = [ x_train2, x_train3]\n",
    "    x_test_list = [x_test2,x_test3]\n",
    "    \n",
    "    val_data_x = []\n",
    "    val_data_y = []\n",
    "    \n",
    "    val_data_x2 = []\n",
    "    val_data_y2 = []\n",
    "\n",
    "    test_data_x = [] \n",
    "    test_data_y = [] \n",
    "\n",
    "    feature_importances = [] \n",
    "    features_used = [] \n",
    "    \n",
    "    auc_curves = []\n",
    "    \n",
    "    \n",
    "    global staged_models\n",
    "    staged_models = []\n",
    "    ind_model_test = []\n",
    "    boot_model_hyperParameters = []\n",
    "    for ij, (x_train, x_test) in enumerate(zip(x_train_list,x_test_list)):\n",
    "        print('On Stage: ' + str(ij) )\n",
    "        \n",
    "        #Standardize \n",
    "        scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "        xt = scaler.transform(x_train)\n",
    " \n",
    "        print('y_train.shape', y_train.shape)\n",
    "        print('xt.shape', xt.shape)\n",
    "\n",
    "        # fs\n",
    "        xt, fs_column = rfe_fs(xt, y_train, fs_type)\n",
    "        x_test = x_test[:,fs_column]\n",
    "        x_train = x_train[:,fs_column]\n",
    "\n",
    "\n",
    "    ################################################################################################### Preprocess\n",
    "\n",
    "        if smote_bol:\n",
    "            oversample = SMOTE(random_state = 123)\n",
    "            xx_train, yy_train = oversample.fit_resample(x_train, y_train)\n",
    "        else:\n",
    "            xx_train = x_train\n",
    "            yy_train = y_train\n",
    "            \n",
    "        global class_weight\n",
    "        class_weight = weights(yy_train)\n",
    "\n",
    "        # Cor \n",
    "        corr_col = find_correlation(xx_train, .8)\n",
    "        xx_train = np.delete(xx_train, corr_col, axis = 1)\n",
    "        nzv = VarianceThreshold(0.01).fit(xx_train)\n",
    "        xx_train = nzv.transform(xx_train)\n",
    "        \n",
    "        x_test = np.delete(x_test, corr_col, axis = 1)\n",
    "        x_test = nzv.transform(x_test)\n",
    "        \n",
    "        #Standardize \n",
    "        scaler = preprocessing.StandardScaler().fit(xx_train)\n",
    "        xx_train = scaler.transform(xx_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "        if spatial == True:\n",
    "            #SPATIAL SIGN\n",
    "            xx_train = preprocessing.normalize(xx_train, norm = 'l2')\n",
    "            x_test = preprocessing.normalize(x_test, norm = 'l2')\n",
    "            \n",
    "        test_data_x.append(x_test)\n",
    "        test_data_y.append(y_test)\n",
    "\n",
    "        if ij == 0:\n",
    "            print('stage 1 fs column', fs_column)\n",
    "            buh = stage1_features[fs_column]\n",
    "            print('stage 1 buh fs', buh)\n",
    "            print('stage 1  cor', corr_col)\n",
    "            buh = np.delete(np.array(buh), corr_col)\n",
    "            print('stage 1 buh cor', buh)\n",
    "            print('stage 1 nzv', np.logical_not(nzv.get_support()))\n",
    "            print('stage 1 deleted',np.delete(buh, np.logical_not(nzv.get_support())))\n",
    "\n",
    "            features_used.append(np.delete(buh, np.logical_not(nzv.get_support())).tolist())   \n",
    "        elif ij == 1:\n",
    "            print('stage 1 fs column', fs_column)\n",
    "\n",
    "            yuh = stage2_features[fs_column]\n",
    "            print('stage 1 buh fs', yuh)\n",
    "            print('stage 1  cor', corr_col)\n",
    "            yuh = np.delete(np.array(yuh), corr_col)\n",
    "            print('stage 1 buh cor', yuh)\n",
    "            print('stage 1 nzv', np.logical_not(nzv.get_support()))\n",
    "            print('stage 1 deleted',np.delete(yuh, np.logical_not(nzv.get_support())))\n",
    "            features_used.append(np.delete(yuh, np.logical_not(nzv.get_support())).tolist()) \n",
    "    \n",
    "\n",
    "    return features_used\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "###### sequential mutli stage hyperparameter tuning functions\n",
    "###################################################################################################\n",
    "def create_sequential_model(hp):\n",
    "    \"\"\"\n",
    "    used to initialize model with specific hyperparameter configuration for search space\n",
    "    \"\"\"\n",
    "   \n",
    "    std1_thresh = hp.Float('std1_thresh', min_value = 0.01, max_value = 0.2, sampling = 'log', default = 0.1) # WAS .01 AJD .2 \n",
    "    halfway_thresh = hp.Float('halfway_thresh', min_value = 0.02, max_value = 0.1, sampling = 'linear', default = 0.05)\n",
    "    model = build_sequential_model(std1_thresh,  halfway_thresh, staged_models )\n",
    "\n",
    "    return model \n",
    "\n",
    "def weighted_func_new2(weight):\n",
    "    \"\"\"\n",
    "    weight - the weight hyperparameter\n",
    "    scaling function applied to fraction of samples retained for first model (penalizes all predictions occuring with all data modalities (ecg + spect))\n",
    "    \"\"\"\n",
    "    return 1 - (1/(math.exp(stg0_weight * weight) + 1))\n",
    "\n",
    "\n",
    "class sequential_Tuner(keras_tuner.engine.tuner.Tuner):\n",
    "    \"\"\"\n",
    "    Defines hyperparameter tuning model for finding multi-stage hyperparameters (std threshold and halfway_thresh)\n",
    "    \"\"\"\n",
    "    def run_trial(self, trial, x, y, **kwargs):\n",
    "        reset_seeds(123)\n",
    "            \n",
    "        model = self.hypermodel.build(trial.hyperparameters)\n",
    "        probs, _, stg_trans = model.evaluate(x)\n",
    "\n",
    "        # compute penalization for std thresholds \n",
    "        \n",
    "        stg1 = stg_trans[0].sum() # number kept during stage 1 \n",
    "            \n",
    "        AUC = metrics.roc_auc_score(y[0], probs)\n",
    "        \n",
    "        weight = weighted_func_new2(stg1 / len(probs))\n",
    "        \n",
    "        weighted_auc = AUC * weight\n",
    "        \n",
    "        \n",
    "        self.oracle.update_trial(trial.trial_id, {'val_acc': weighted_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seeds(123)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import time \n",
    "from sklearn.base import clone\n",
    "import pickle\n",
    "\n",
    "num_class = 2\n",
    "\n",
    "stage_1 = ['NYHA_2', 'NYHA_3', 'NYHA_4', 'Race_1', 'Race_2', 'Race_3', 'Race_4','Race_5', 'Age',  'ACEI_or_ARB', 'CABG', 'CAD', 'DM','Gender', 'HTN',  'MI', 'PCI', 'Smoking']\n",
    "stage_2 = ['LBBB', 'ECG_pre_QRSd' ]\n",
    "stage_3 = ['SPECT_pre_LVEF', 'SPECT_pre_ESV','SPECT_pre_EDV', 'SPECT_pre_WTper', 'SPECT_pre_PSD', 'SPECT_pre_PBW', 'SPECT_pre_50scar', 'SPECT_pre_gMyoMass', 'SPECT_pre_WTsum',\n",
    "            'SPECT_pre_StrokeVolume', 'SPECT_pre_PhaseKurt','SPECT_pre_Diastolic_PhasePeak', 'SPECT_pre_Diastolic_PhaseSkew','SPECT_pre_Diastolic_PBW', 'SPECT_pre_Diastolic_PSD', 'SPECT_pre_EDSI',\n",
    "            'SPECT_pre_EDE', 'SPECT_pre_ESSI', 'SPECT_pre_ESE','SPECT_pre_Diastolic_PhaseKurt', 'SPECT_pre_PhasePeak', 'PECT_pre_SRscore',  'Concordance']\n",
    "\n",
    "\n",
    "class Bootstraps:\n",
    "    \"\"\"\n",
    "    n_models - # of models in ensemble\n",
    "    model_itself - base model to use (RF, svm, enet, etc...) \n",
    "    sampling_fraction - fraction of samples for psuedo-bootstrap\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_x, data_y, n_models, model_itself, sampling_fraction):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.n_models = n_models\n",
    "        self.model_itself = model_itself\n",
    "        self.sampling_fraction = sampling_fraction\n",
    "        self.list_of_models = []\n",
    "\n",
    "    def fit(self): \n",
    "        fitted_models = []\n",
    "        for num in range(self.n_models):\n",
    "            reset_seeds(123 + num)  \n",
    "            model = clone(self.model_itself)\n",
    "            np.random.seed(123 + num) \n",
    "\n",
    "            resample_index = np.random.choice(np.arange(len(self.data_y)), size = round(self.sampling_fraction * len(self.data_y)), replace = True).tolist()\n",
    "\n",
    "            model.fit(self.data_x[resample_index], self.data_y[resample_index]);\n",
    "            fitted_models.append(model)\n",
    "            \n",
    "            del model, resample_index\n",
    "        \n",
    "        self.list_of_models = fitted_models\n",
    "        #return fitted_models\n",
    "    \n",
    "    def evaluate(self, test_data_x):\n",
    "        probabilities = []\n",
    "        for trained_model in self.list_of_models:\n",
    "            prob_predictions = trained_model.predict_proba(test_data_x)\n",
    "            probabilities.append(prob_predictions[:,1])\n",
    "        \n",
    "        probabilities = np.array(probabilities)\n",
    "        avg_probabilities = probabilities.mean(axis = 0)\n",
    "        avg_class = np.where(avg_probabilities >= 0.50, 1, 0)\n",
    "        std = probabilities.std(axis = 0)\n",
    "        \n",
    "        class_counts = probabilities.round()\n",
    "        num_response = np.sum(class_counts, axis = 0 )\n",
    "        num_majority_class = np.where(num_response <= self.n_models/2, self.n_models - num_response, num_response)\n",
    "        \n",
    "        return avg_probabilities, avg_class, std, num_majority_class\n",
    "    \n",
    "    def return_importance(self):\n",
    "        importance = [] \n",
    "        for trained_model in self.list_of_models:\n",
    "            result = permutation_importance(trained_model,  self.data_x, self.data_y, n_repeats=10,\n",
    "                                 random_state=123)\n",
    "            importance.append(result.importances_mean)\n",
    "            \n",
    "        importance = np.array(importance).mean(axis = 0)\n",
    "        return importance\n",
    "    \n",
    "\n",
    "class build_sequential_model():\n",
    "    \"\"\"\n",
    "    std1_thresh - for deciding rules\n",
    "    halfway_thresh - for deciding rules\n",
    "    model_list - list of length 2 for the 2 stages which contains the ensemble models\n",
    "    \"\"\"\n",
    "    def __init__(self, std1_thresh,halfway_thresh, model_list):\n",
    "        \n",
    "        self.std1_thresh = std1_thresh\n",
    "        self.model_list = model_list\n",
    "        self.halfway_thresh = halfway_thresh\n",
    "        \n",
    "    def evaluate(self, test_data_list):\n",
    "        \"\"\"\n",
    "        aggregated_probabilities = final probabilities which stem from either the first or second model based off the rules\n",
    "        keep = whether or not the sample should be predicted using the first model ( 1 = predict using first model, i.e., the sample did not need spect data)\n",
    "        \"\"\"\n",
    "        print('test_data_list[0].shape',test_data_list[0].shape)\n",
    "        aggregated_probabilities = np.zeros(shape = test_data_list[0].shape[0])\n",
    "        keep = []\n",
    "        standard_deviations = []\n",
    "        for stage, (modell, data) in enumerate(zip(self.model_list, test_data_list)):\n",
    "            print('stage', stage, 'date shape', data.shape)\n",
    "            if stage == 0:\n",
    "                avg_probabilities, _, std, _ = modell.evaluate(data)\n",
    "                keep.append(np.where(np.logical_and(std <= self.std1_thresh, abs(avg_probabilities - 0.5) > self.halfway_thresh), 1, 0))\n",
    "                aggregated_probabilities += np.where(keep[0] == 1, avg_probabilities, 0)\n",
    "                standard_deviations.append(std)\n",
    "                \n",
    "            elif stage == 1:\n",
    "                avg_probabilities, _, std, _ = modell.evaluate(data)\n",
    "                aggregated_probabilities += np.where(keep[0] == 1, 0, avg_probabilities)\n",
    "                standard_deviations.append(std)\n",
    "\n",
    "        return aggregated_probabilities, standard_deviations, keep\n",
    "    \n",
    "    \n",
    "stage_1 = ['NYHA_2', 'NYHA_3', 'NYHA_4', 'Race_1', 'Race_2', 'Race_3', 'Race_4','Race_5', 'Age',  'ACEI_or_ARB', 'CABG', 'CAD', 'DM','Gender', 'HTN',  'MI', 'PCI', 'Smoking']\n",
    "stage_2 = ['LBBB', 'ECG_pre_QRSd' ]\n",
    "stage_3 = ['SPECT_pre_LVEF', 'SPECT_pre_ESV','SPECT_pre_EDV', 'SPECT_pre_WTper', 'SPECT_pre_PSD', 'SPECT_pre_PBW', 'SPECT_pre_50scar', 'SPECT_pre_gMyoMass', 'SPECT_pre_WTsum',\n",
    "            'SPECT_pre_StrokeVolume', 'SPECT_pre_PhaseKurt','SPECT_pre_Diastolic_PhasePeak', 'SPECT_pre_Diastolic_PhaseSkew','SPECT_pre_Diastolic_PBW', 'SPECT_pre_Diastolic_PSD', 'SPECT_pre_EDSI',\n",
    "            'SPECT_pre_EDE', 'SPECT_pre_ESSI', 'SPECT_pre_ESE','SPECT_pre_Diastolic_PhaseKurt', 'SPECT_pre_PhasePeak', 'PECT_pre_SRscore',  'Concordance']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## Call experiment via this script ##############################\n",
    "\n",
    "def score(dataframe,  save_path,fs_method, smotee, spatial_sign, what_model  ):\n",
    "    \"\"\"\n",
    "   Main function for running experiement \n",
    "    fs_method - feature selection method \n",
    "    smotee - if to use SMOTE\n",
    "    spatial_sign - if to use spatial sign \n",
    "    what_model - what base model to use (RF, svm, etc...)\n",
    "    \"\"\"\n",
    "\n",
    "    clinical_data = dataframe.drop(columns = ['resp', 'Race', 'NYHA', 'Hospitalization'])\n",
    "    \n",
    "    \n",
    "    # making data for different stages \n",
    "    X2 = np.array(clinical_data.filter(stage_1 + stage_2))\n",
    "    X3 = np.array(clinical_data.filter(stage_1 + stage_2 + stage_3))\n",
    " \n",
    "    # define global features \n",
    "    global stage1_features\n",
    "    stage1_features = clinical_data.filter(stage_1 + stage_2).columns\n",
    " \n",
    "    global stage2_features \n",
    "    stage2_features = clinical_data.filter(stage_1 + stage_2 + stage_3).columns\n",
    "    \n",
    "    # foprmat data\n",
    "    Y = np.where(np.array(response)  == 'yes', 1, 0)    \n",
    "   \n",
    "    # evaluate ML\n",
    "    stage0_wt = [1] # weight parameter for scaling function for weighted auc \n",
    "    stage1_wt = [1]\n",
    "    for weights in product(stage0_wt, stage1_wt):\n",
    "\n",
    "        combo_output = []\n",
    "        scores = np.zeros((10,5))\n",
    "        idx = 0\n",
    "        ind_scores = [] \n",
    "        sequential_mods = []\n",
    "        stand_dev = []\n",
    "        boot_model_hypers = []\n",
    "        stage_transitions = []\n",
    "        val_stand_dev = []\n",
    "        val_stage_transitions = []\n",
    "        scle_weights = []\n",
    "        ft_import = []\n",
    "        auc_curves = []\n",
    "        ft_used = []\n",
    "        crnt_fold = 0 \n",
    "        for train, test in StratifiedKFold(n_splits=10, shuffle = True, random_state = 123).split(X2, Y):\n",
    "            print(len(train))\n",
    "            print('Fold', idx)\n",
    "            features = train_evaluate( X2[train], X3[train], Y[train],  X2[test], X3[test], Y[test], train, test, fs_type = fs_method, smote_bol = smotee , spatial = spatial_sign, models = what_model, fold =  crnt_fold)\n",
    "            ft_used.append(features)\n",
    "            idx += 1\n",
    "            crnt_fold += 1\n",
    "        #scores.to_csv(save_path , mode='a', header=not os.path.exists(save_path ), index = False)\n",
    "    \n",
    "        data_run = [ ft_used]\n",
    "   \n",
    "        with open(what_model + '_' +  str(1) + '_' +  str(1) + 'features', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(data_run, fp)\n",
    "   \n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining bootstrap ensemble and multi stage sequential classes and script for calling the above main loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seeds(123)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import time \n",
    "from sklearn.base import clone\n",
    "import pickle\n",
    "\n",
    "num_class = 2\n",
    "\n",
    "stage_1 = ['NYHA_2', 'NYHA_3', 'NYHA_4', 'Race_1', 'Race_2', 'Race_3', 'Race_4','Race_5', 'Age',  'ACEI_or_ARB', 'CABG', 'CAD', 'DM','Gender', 'HTN',  'MI', 'PCI', 'Smoking']\n",
    "stage_2 = ['LBBB', 'ECG_pre_QRSd' ]\n",
    "stage_3 = ['SPECT_pre_LVEF', 'SPECT_pre_ESV','SPECT_pre_EDV', 'SPECT_pre_WTper', 'SPECT_pre_PSD', 'SPECT_pre_PBW', 'SPECT_pre_50scar', 'SPECT_pre_gMyoMass', 'SPECT_pre_WTsum',\n",
    "            'SPECT_pre_StrokeVolume', 'SPECT_pre_PhaseKurt','SPECT_pre_Diastolic_PhasePeak', 'SPECT_pre_Diastolic_PhaseSkew','SPECT_pre_Diastolic_PBW', 'SPECT_pre_Diastolic_PSD', 'SPECT_pre_EDSI',\n",
    "            'SPECT_pre_EDE', 'SPECT_pre_ESSI', 'SPECT_pre_ESE','SPECT_pre_Diastolic_PhaseKurt', 'SPECT_pre_PhasePeak', 'PECT_pre_SRscore',  'Concordance']\n",
    "\n",
    "\n",
    "class Bootstraps:\n",
    "    \"\"\"\n",
    "    n_models - # of models in ensemble\n",
    "    model_itself - base model to use (RF, svm, enet, etc...) \n",
    "    sampling_fraction - fraction of samples for psuedo-bootstrap\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_x, data_y, n_models, model_itself, sampling_fraction):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.n_models = n_models\n",
    "        self.model_itself = model_itself\n",
    "        self.sampling_fraction = sampling_fraction\n",
    "        self.list_of_models = []\n",
    "\n",
    "    def fit(self): \n",
    "        fitted_models = []\n",
    "        for num in range(self.n_models):\n",
    "            reset_seeds(123 + num)  \n",
    "            model = clone(self.model_itself)\n",
    "            np.random.seed(123 + num) \n",
    "\n",
    "            resample_index = np.random.choice(np.arange(len(self.data_y)), size = round(self.sampling_fraction * len(self.data_y)), replace = True).tolist()\n",
    "\n",
    "            model.fit(self.data_x[resample_index], self.data_y[resample_index]);\n",
    "            fitted_models.append(model)\n",
    "            \n",
    "            del model, resample_index\n",
    "        \n",
    "        self.list_of_models = fitted_models\n",
    "        #return fitted_models\n",
    "    \n",
    "    def evaluate(self, test_data_x):\n",
    "        probabilities = []\n",
    "        for trained_model in self.list_of_models:\n",
    "            prob_predictions = trained_model.predict_proba(test_data_x)\n",
    "            probabilities.append(prob_predictions[:,1])\n",
    "        \n",
    "        probabilities = np.array(probabilities)\n",
    "        avg_probabilities = probabilities.mean(axis = 0)\n",
    "        avg_class = np.where(avg_probabilities >= 0.50, 1, 0)\n",
    "        std = probabilities.std(axis = 0)\n",
    "        \n",
    "        class_counts = probabilities.round()\n",
    "        num_response = np.sum(class_counts, axis = 0 )\n",
    "        num_majority_class = np.where(num_response <= self.n_models/2, self.n_models - num_response, num_response)\n",
    "        \n",
    "        return avg_probabilities, avg_class, std, num_majority_class\n",
    "    \n",
    "    def return_importance(self):\n",
    "        importance = [] \n",
    "        for trained_model in self.list_of_models:\n",
    "            result = permutation_importance(trained_model,  self.data_x, self.data_y, n_repeats=10,\n",
    "                                 random_state=123)\n",
    "            importance.append(result.importances_mean)\n",
    "            \n",
    "        importance = np.array(importance).mean(axis = 0)\n",
    "        return importance\n",
    "    \n",
    "\n",
    "class build_sequential_model():\n",
    "    \"\"\"\n",
    "    std1_thresh - for deciding rules\n",
    "    halfway_thresh - for deciding rules\n",
    "    model_list - list of length 2 for the 2 stages which contains the ensemble models\n",
    "    \"\"\"\n",
    "    def __init__(self, std1_thresh,halfway_thresh, model_list):\n",
    "        \n",
    "        self.std1_thresh = std1_thresh\n",
    "        self.model_list = model_list\n",
    "        self.halfway_thresh = halfway_thresh\n",
    "        \n",
    "    def evaluate(self, test_data_list):\n",
    "        \"\"\"\n",
    "        aggregated_probabilities = final probabilities which stem from either the first or second model based off the rules\n",
    "        keep = whether or not the sample should be predicted using the first model ( 1 = predict using first model, i.e., the sample did not need spect data)\n",
    "        \"\"\"\n",
    "        print('test_data_list[0].shape',test_data_list[0].shape)\n",
    "        aggregated_probabilities = np.zeros(shape = test_data_list[0].shape[0])\n",
    "        keep = []\n",
    "        standard_deviations = []\n",
    "        for stage, (modell, data) in enumerate(zip(self.model_list, test_data_list)):\n",
    "            print('stage', stage, 'date shape', data.shape)\n",
    "            if stage == 0:\n",
    "                avg_probabilities, _, std, _ = modell.evaluate(data)\n",
    "                keep.append(np.where(np.logical_and(std <= self.std1_thresh, abs(avg_probabilities - 0.5) > self.halfway_thresh), 1, 0))\n",
    "                aggregated_probabilities += np.where(keep[0] == 1, avg_probabilities, 0)\n",
    "                standard_deviations.append(std)\n",
    "                \n",
    "            elif stage == 1:\n",
    "                avg_probabilities, _, std, _ = modell.evaluate(data)\n",
    "                aggregated_probabilities += np.where(keep[0] == 1, 0, avg_probabilities)\n",
    "                standard_deviations.append(std)\n",
    "\n",
    "        return aggregated_probabilities, standard_deviations, keep\n",
    "    \n",
    "    \n",
    "stage_1 = ['NYHA_2', 'NYHA_3', 'NYHA_4', 'Race_1', 'Race_2', 'Race_3', 'Race_4','Race_5', 'Age',  'ACEI_or_ARB', 'CABG', 'CAD', 'DM','Gender', 'HTN',  'MI', 'PCI', 'Smoking']\n",
    "stage_2 = ['LBBB', 'ECG_pre_QRSd' ]\n",
    "stage_3 = ['SPECT_pre_LVEF', 'SPECT_pre_ESV','SPECT_pre_EDV', 'SPECT_pre_WTper', 'SPECT_pre_PSD', 'SPECT_pre_PBW', 'SPECT_pre_50scar', 'SPECT_pre_gMyoMass', 'SPECT_pre_WTsum',\n",
    "            'SPECT_pre_StrokeVolume', 'SPECT_pre_PhaseKurt','SPECT_pre_Diastolic_PhasePeak', 'SPECT_pre_Diastolic_PhaseSkew','SPECT_pre_Diastolic_PBW', 'SPECT_pre_Diastolic_PSD', 'SPECT_pre_EDSI',\n",
    "            'SPECT_pre_EDE', 'SPECT_pre_ESSI', 'SPECT_pre_ESE','SPECT_pre_Diastolic_PhaseKurt', 'SPECT_pre_PhasePeak', 'PECT_pre_SRscore',  'Concordance']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## Call experiment via this script ##############################\n",
    "\n",
    "def score(dataframe,  save_path,fs_method, smotee, spatial_sign, what_model  ):\n",
    "    \"\"\"\n",
    "   Main function for running experiement \n",
    "    fs_method - feature selection method \n",
    "    smotee - if to use SMOTE\n",
    "    spatial_sign - if to use spatial sign \n",
    "    what_model - what base model to use (RF, svm, etc...)\n",
    "    \"\"\"\n",
    "\n",
    "    clinical_data = dataframe.drop(columns = ['resp', 'Race', 'NYHA', 'Hospitalization'])\n",
    "    \n",
    "    \n",
    "    # making data for different stages \n",
    "    X2 = np.array(clinical_data.filter(stage_1 + stage_2))\n",
    "    X3 = np.array(clinical_data.filter(stage_1 + stage_2 + stage_3))\n",
    " \n",
    "    # define global features \n",
    "    global stage1_features\n",
    "    stage1_features = clinical_data.filter(stage_1 + stage_2).columns\n",
    " \n",
    "    global stage2_features \n",
    "    stage2_features = clinical_data.filter(stage_1 + stage_2 + stage_3).columns\n",
    "    \n",
    "    # foprmat data\n",
    "    Y = np.where(np.array(response)  == 'yes', 1, 0)    \n",
    "   \n",
    "    # evaluate ML\n",
    "    stage0_wt = [1] # weight parameter for scaling function for weighted auc \n",
    "    stage1_wt = [1]\n",
    "    for weights in product(stage0_wt, stage1_wt):\n",
    "\n",
    "        combo_output = []\n",
    "        scores = np.zeros((10,5))\n",
    "        idx = 0\n",
    "        ind_scores = [] \n",
    "        sequential_mods = []\n",
    "        stand_dev = []\n",
    "        boot_model_hypers = []\n",
    "        stage_transitions = []\n",
    "        val_stand_dev = []\n",
    "        val_stage_transitions = []\n",
    "        scle_weights = []\n",
    "        ft_import = []\n",
    "        auc_curves = []\n",
    "        ft_used = []\n",
    "        crnt_fold = 0 \n",
    "        for train, test in StratifiedKFold(n_splits=10, shuffle = True, random_state = 123).split(X2, Y):\n",
    "            print(len(train))\n",
    "            print('Fold', idx)\n",
    "            scores[idx], ind_performance, mods, stds, boot_hp, stage_trans, std_val, stage_trans_val, scaling_weight, feature_importances, auc_curv, feature_names = train_evaluate( X2[train], X3[train], Y[train],  X2[test], X3[test], Y[test], train, test, fs_type = fs_method, smote_bol = smotee , spatial = spatial_sign, models = what_model, fold =  crnt_fold)\n",
    "            idx += 1\n",
    "            ind_scores.append(ind_performance)\n",
    "            sequential_mods.append(mods)\n",
    "            stand_dev.append(stds)\n",
    "            boot_model_hypers.append(boot_hp)\n",
    "            stage_transitions.append(stage_trans)\n",
    "            val_stand_dev.append(std_val)\n",
    "            val_stage_transitions.append(stage_trans_val)\n",
    "            scle_weights.append(scaling_weight)\n",
    "            ft_import.append(feature_importances)\n",
    "            auc_curves.append(auc_curv)\n",
    "            ft_used.append(feature_names)\n",
    "            crnt_fold += 1\n",
    "        scores = pd.DataFrame(scores)\n",
    "        #scores.to_csv(save_path , mode='a', header=not os.path.exists(save_path ), index = False)\n",
    "    \n",
    "        data_run = [scores, ind_scores, sequential_mods, stand_dev, boot_model_hypers, stage_transitions, val_stand_dev, val_stage_transitions, scle_weights, ft_import, auc_curves, ft_used]\n",
    "   \n",
    "        with open(what_model + '_' +  str(1) + '_' +  str(1) + 'final_please', \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(data_run, fp)\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with smote and spatial sign (repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(df, save_path = '/root/uncertainty_multi_stage/enet_weights_new_weight_func.csv', fs_method = 'lr', smotee = True, spatial_sign = True, what_model = 'enet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# Processing pickle files for displaying raw results \n",
    "# =======================================\n",
    "\n",
    "\n",
    "with open('/root/uncertainty_multi_stage/enet_1_1final_please', \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "score = b[0].rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "print('Avg score',score.mean(axis = 0))\n",
    "print('std score',score.std(axis = 0))\n",
    "\n",
    "print('Raw Scores', score)\n",
    "\n",
    "print('Model scores')\n",
    "for model in range(2):\n",
    "    model_scores = []\n",
    "    for cv in range(10):\n",
    "        ind = b[1][cv][model] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "        model_scores.append(ind)\n",
    "        print('CV ', cv,' Model: ', model, 'Score: ', ind)\n",
    "    print('             Model ', model, ' Avg ', np.array(model_scores).mean(axis = 0 ))\n",
    "    print('             Model ', model, ' std ', np.array(model_scores).std(axis = 0 ))\n",
    "\n",
    "print('std thresholds')\n",
    "for cv in range(10):\n",
    "    print('CV ', cv, ' std1_thresh', (vars(b[2][cv]).get('std1_thresh')))\n",
    "    print('CV ', cv, ' midway thresh', (vars(b[2][cv]).get('halfway_thresh')))\n",
    "    \n",
    "print('std')\n",
    "for model in range(2):\n",
    "    std_overall = []\n",
    "    for cv in range(10):\n",
    "        ind = b[3][cv][model] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "        std_overall.append(ind)\n",
    "        print('CV ', cv,' Model: ', model, 'Score: ', ind)\n",
    "    print('             Model ', model, ' Avg ', ((np.array(std_overall[:8]).sum() + np.array(std_overall[8:]).sum())/(np.array(std_overall[:8]).size + np.array(std_overall[:8]).size)))\n",
    "    \n",
    "print('hyperparams')\n",
    "for model in range(2):\n",
    "    boot_MODELS = []\n",
    "    for cv in range(10):\n",
    "        ind = b[4][cv][model] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "        boot_MODELS.append(ind)\n",
    "        print('CV ', cv,' Model: ', model, 'Score: ', ind)\n",
    "    print('             Model ', model, ' Avg ', np.array(boot_MODELS).mean(axis = 0 ))\n",
    "    \n",
    "print('stg movements')\n",
    "for model in range(1):\n",
    "    std_overall = []\n",
    "    for cv in range(10):\n",
    "        ind = b[5][cv][model] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "        std_overall.append(ind)\n",
    "        print('CV ', cv,' Model: ', model, 'Score: ', ind)\n",
    "    print('             Model ', model, ' Avg ', (np.array(std_overall[:8]).sum() + np.array(std_overall[8:]).sum()/(np.array(std_overall[:8]).size + np.array(std_overall[:8]).size)))\n",
    "    \n",
    "    \n",
    "print('scaling weight function')\n",
    "for cv in range(10):\n",
    "    ind = b[8][cv] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "    print('CV ', cv, ' Scaling Weight: ', ind)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stg movements')\n",
    "\n",
    "std_overall = []\n",
    "SUMS = []   \n",
    "for cv in range(10):\n",
    "    ind = b[5][cv][0] #.rename({0:'AUC',1:'Accuracy',2:'Kappa',3:'Sensitivity',4:'specificity'}, axis = 1)\n",
    "    std_overall.append(ind)\n",
    "    SUMS.append(sum(ind)/len(ind))\n",
    "    print('CV ', cv,' Model: ', model, 'Score: ', ind)\n",
    "print('percent of patients requiring spect', 100 - np.mean(SUMS) * 100)\n",
    "print(np.std(SUMS) * 100)\n",
    "print('25th percentile', np.percentile(np.array(SUMS), 25) * 100)\n",
    "print('50th percentile', np.percentile(np.array(SUMS), 50)* 100)\n",
    "print('75th percentile', np.percentile(np.array(SUMS), 75)* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_roc_curve = []\n",
    "for i in range(10):\n",
    "    list_of_roc_curve.append(b[10][i][2]) # vary last slice from 0, 1, 2 for ensemble 1, ensemble 2, and multistage \n",
    "    \n",
    "from sklearn.metrics import auc\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for i in range(10):\n",
    "    viz = list_of_roc_curve[i]\n",
    "    viz.plot(ax = ax, alpha = 0.3)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Multi-stage)\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
